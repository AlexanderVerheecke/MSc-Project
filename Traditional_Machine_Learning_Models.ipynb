{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexanderVerheecke/TwitterSentimentAnalysis/blob/main/Traditional_Machine_Learning_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "213PQUAHlOr0"
      },
      "source": [
        "How to run:\n",
        "The dataset used in this colab file is taken from my personal google Drive folder. I was unable to link the gitlab file to here. If the system is not connected to my Google Drive folder, the user will need to download the \n",
        "datasets themselves from : https://drive.google.com/drive/folders/1hxBtAXu-IfoajBtJIG_8q7lFcmcLWYJu?usp=sharing \n",
        "- SemEval data: SemEval 2017 -> SemEval2017_DataSet.csv\n",
        "- English: OwnTweets -> English -> latestEnglish.csv\n",
        "- English translation: OwnTweets -> latestGermanTranlatedToEnglish.csv\n",
        "- DAI data: DAI TU Berlin -> de_sentiment_UNIQUE.csv\n",
        "- German: OwnTweets -> German -> latestGerman.csv\n",
        "- German translation: OwnTweets -> German -> latestEnglishTranslatedToGERMAN.csv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The datasets will need to be uploaded to Colabs files folder on left and the file path copied to the respective dataset reading. Once all data is correctly loaded, the user will simply need to 'run all' under 'run time'.\n",
        "\n",
        "Under \" MODEL TRAINING AND PREDICTION \", the models will be trained with the datasets and output their performance in form of a classification report.\n",
        "\n",
        "A comparison of the best performing model's (SVM) predictions and true labels can be seen under ' PREDICTION COMPARISON WITH TRUE LABELS '. It has two subsections 'GERMAN VS ENGLISH TRANSLATION\" and \"ENGLISH VS GERMAN TRANSLATION\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkz6lSlzoLM4"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udO3UTskoJjz",
        "outputId": "dc077930-bc5c-4237-f02e-4fac4e4176ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# nltk and its downloads\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# sklearn and the various models to train on training data\n",
        "from sklearn import svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# to evaluate the models\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jY4djdsLViZ"
      },
      "source": [
        "# Reading SemEval and German_Unqiue dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScKbS9u-RQzS"
      },
      "outputs": [],
      "source": [
        "# Reading data to be trained on\n",
        "\n",
        "SemEval = pd.read_csv('/content/drive/MyDrive/Education/University/Master/Classes/Thesis/Data/SemEval2017/SemEval2017_DataSet.csv')\n",
        "SemEval = pd.DataFrame(SemEval)\n",
        "\n",
        "DAI = pd.read_csv('/content/drive/MyDrive/Education/University/Master/Classes/Thesis/Data/DAI TU Berlin/de_sentiment_UNIQUE.csv')\n",
        "DAI = pd.DataFrame(DAI)\n",
        "\n",
        "\n",
        "# Reading german labelled and its English translation\n",
        "German = pd.read_csv('/content/drive/MyDrive/Education/University/Master/Classes/Thesis/Data/OwnTweets/German/latestGerman.csv')\n",
        "German = pd.DataFrame(German)\n",
        "\n",
        "German_translated = pd.read_csv('/content/drive/MyDrive/Education/University/Master/Classes/Thesis/Data/OwnTweets/latestGermanTranlatedToEnglish.csv')\n",
        "German_translated = pd.DataFrame(German_translated)\n",
        "\n",
        "# Reading english labelled and its German translation\n",
        "English = pd.read_csv('/content/drive/MyDrive/Education/University/Master/Classes/Thesis/Data/OwnTweets/English/latestEnglish.csv')\n",
        "English = pd.DataFrame(English)\n",
        "\n",
        "English_translated = pd.read_csv('/content/drive/MyDrive/Education/University/Master/Classes/Thesis/Data/OwnTweets/German/latestEnglishTranslatedToGERMAN.csv')\n",
        "English_translated = pd.DataFrame(English_translated)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwaSL96aR167"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ264HaqR0fa",
        "outputId": "192ca9d4-8562-491e-d327-3a2fcc94a337"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem.cistem import Cistem\n",
        "from nltk.stem import *\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "stemmer_ENG = PorterStemmer()\n",
        "stemmer_GER = Cistem()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Some of the functionality of the tweet_to_words functions have been inspried by https://gist.github.com/arybanary/4d6f7596825a4c95d74d9ec1597daefd#file-stemming_tweets-py . \n",
        "Additional functionalities has been added to it.\n",
        "Extra addittions for processing function:\n",
        "- remove symbols and single German character\n",
        "- expand English contractions\n",
        "\n",
        "\"\"\"\n",
        "remove_symbols = '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])'  #emojis, symbols, and punctuation\n",
        "remove_symbols_GER = '(@[A-Za-z0-9]+)|([^0-9A-Z√Ñ√ú√ñ·∫ûa-z√§√º√∂√ü \\t])'  #emojis, symbols, and punctuation BUT KEEPING GERMAN SPECIAL CHARS\n",
        "single_CHAR_GER = '(^| ).( |$)' #matches any single character between spaces\n",
        "\n",
        "# removing negation from stopwords in German and English\n",
        "Ger_stop = stopwords.words(\"german\")\n",
        "Ger_negation = ['nicht', 'nichts', 'keine', 'keinen', ]\n",
        "for i in Ger_negation:\n",
        "  Ger_stop.remove(i)\n",
        "\n",
        "english_stop = stopwords.words('english')\n",
        "english_negative = ['nor', 'not']\n",
        "for i in english_negative:\n",
        "  english_stop.remove(i)\n",
        "\n",
        "#function to expand English contractions to their full form. Any contraction matching this will be expanded to WORD and EXPANSION i.e., you're -> you are\n",
        "def expand(text): \n",
        "    \n",
        "    # specific\n",
        "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    # general\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"wanna\",\"want to\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def tweet_to_words_GER(text):\n",
        "    # print(\"Original: \",text)\n",
        "    text = text.lower() # Convert to lower case\n",
        "    # print(\"Lowered: \", text)\n",
        "    text = re.sub(r\"(http\\S+)|(www\\S+)\", \"XXXURLXXX\", text) # Replace links with meaningless URL indicator\n",
        "    # print(\"Links: \",text)\n",
        "    text = re.sub(r\"#\", \"\", text) # Remove '#' in front of hashtags so the words following the hashtag can still be analysed\n",
        "    # print(\"Hashtag: \", text)\n",
        "    text = re.sub(r\"@\\S+\", \"XXXUSERNAMEXXX\", text) # Replace mentions with meaningless Username indicator\n",
        "    # print(\"Mentions: \", text)\n",
        "    text = re.sub(remove_symbols_GER,\" \", text) # removes symbols\n",
        "    # print(\"Symbols: \",text)\n",
        "    text = re.sub(\"(?:(?<=^)|(?<=\\s))(\\d+[.,]*)+(?=$|\\s)\", \"\", text) # Remove all numbers not being part of alphanumeric word\n",
        "    # print(\"non-alpha: \",text)\n",
        "    text = re.sub(r\"rt \", \"\", text) # Remove 'RT'\n",
        "    # print(\"RT: \", text)  \n",
        "    text = re.sub(single_CHAR_GER, \" \", text)\n",
        "    # print(\"Single CHAR: \", text)\n",
        "    words = tweet_tokenizer.tokenize(text)\n",
        "    # print(\"Tokenised: \",words)\n",
        "    words = [w for w in words if w not in Ger_stop] # Remove stopwords\n",
        "    # print(\"Stopwords: \", words)\n",
        "    words = [stemmer_GER.stem(w) for w in words] # Stem\n",
        "    \n",
        "    return words\n",
        "  \n",
        "def tweet_to_words_ENG(text):\n",
        "    # print(\"Original text: \", text)\n",
        "    text = text.lower() # Convert to lower case\n",
        "    # print(\"Lowered: \", text)\n",
        "    text = re.sub(r\"(http\\S+)|(www\\S+)\", \"XXXURLXXX\", text) # Replace links with meaningless URL indicator\n",
        "    # print(\"Links: \", text)\n",
        "    text = re.sub(r\"#\", \"\", text) # Remove '#' in front of hashtags so the words following the hashtag can still be analysed\n",
        "    # print(\"Hashtag: \", text)\n",
        "    text = re.sub(r\"@\\S+\", \"XXXUSERNAMEXXX\", text) # Replace mentions with meaningless Username indicator\n",
        "    # print(\"Mentions: \", text)\n",
        "    text = re.sub(remove_symbols,\" \", text) # removes symbols\n",
        "    # print(\"Symbols: \",text)\n",
        "    text = expand(text) #expands contractions\n",
        "    # print(\"Expand: \", text)\n",
        "    text = re.sub(\"(?:(?<=^)|(?<=\\s))(\\d+[.,]*)+(?=$|\\s)\", \"\", text) # Remove all numbers not being part of alphanumeric word\n",
        "    # print(\"non-Alpha: \", text)\n",
        "    text = re.sub(r\"rt \", \"\", text) # Remove 'RT'\n",
        "    # print(\"RT: \", text)\n",
        "    words = tweet_tokenizer.tokenize(text)\n",
        "    words = [w for w in words if w not in english_stop] # Remove stopwords\n",
        "    words = [stemmer_ENG.stem(w) for w in words] # Stem\n",
        "    \n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8G38rXtwkUlj"
      },
      "outputs": [],
      "source": [
        "SemEval['clean'] = SemEval[\"Tweet\"].apply(tweet_to_words_ENG)\n",
        "DAI['clean'] = DAI['Tweet'].apply(tweet_to_words_GER)\n",
        "\n",
        "\n",
        "# creates a randomly shuffled dataset of SemEval and DAI\n",
        "# randomness results in different results each time due to different train/test but difference is really insignificant\n",
        "combinedData = SemEval.append(DAI, ignore_index=True)\n",
        "combinedData = combinedData.sample(frac=1)\n",
        "\n",
        "German['clean'] = German['Tweet'].apply(tweet_to_words_GER)\n",
        "German_translated['clean'] = German_translated['Tweet'].apply(tweet_to_words_ENG)\n",
        "\n",
        "English['clean'] = English[\"Tweet\"].apply(tweet_to_words_ENG)\n",
        "\n",
        "English_translated['clean'] = English_translated[\"Tweet\"].apply(tweet_to_words_GER)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YMzK4NUkQYQ"
      },
      "source": [
        "#Cleaning function demonstration for demo\n",
        "\n",
        "Uncomment in above two cleaning functions to show difference:\n",
        "- print(\"Original\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPGVVM-XW04x",
        "outputId": "ed1089e8-be8f-4f7f-83e0-b37bae7d75a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['left', 'realli', 'gone', 'full', 'retard']\n",
            "['xxxusernamexxx', 'xxxusernamexxx', 'xxxusernamexxx', 'xxxusernamexxx', 'mani', 'vote', 'gop', 'want', 'plan', 'social', 'secur', 'job']\n",
            "['motiv', 'deep', 'learn', 'resili', 'help', 'youth', 'develop', 'initi', 'great', 'tip', 'xxxusernamexxx']\n"
          ]
        }
      ],
      "source": [
        "to_clean_ENG = [\"The left has really gone Full retard haven't they?\", \"@user @user @user @user many didn't vote GOP because they wanted a plan for social security and jobs.\", \"To motivate deep learning & resilience help youth develop initiative. Great tips from @user\"]\n",
        "for i in to_clean_ENG:\n",
        "  print(tweet_to_words_ENG(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghb5DjY4dUUW",
        "outputId": "3b455713-dbc6-464e-9f3a-dd5098b6e067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['einfach', 'schon', 'kotz', 'heul', 'bes', 'abwechsel', 'scheiss']\n",
            "['zeit', 'heu', 'ebook', 'read', 'sony', 'mal', 'tes', 'beruflich', 'naturlich']\n",
            "['zeit', 'vergeh', 'spass']\n"
          ]
        }
      ],
      "source": [
        "to_clean_GER = [\"Wie ich einfach schon wieder kotzen k√∂nnte. Und heulen. Am Besten abwechselnd. Schei√üe.\", \"Wenn Zeit ist werde ich heute einen eBook-Reader von Sony mal testen ( beruflich nat√ºrlich)\", \"Wie die Zeit vergeht; wenn man Spa√ü hat.\"]\n",
        "for i in to_clean_GER:\n",
        "  print(tweet_to_words_GER(i))\n",
        "\n",
        "# ['vollerei', 'lass', 'gruss', 'xxxusernametokenxxx', 'wirklich', 'langweil', 'nich', 'schlaf', ',', 'xxxurltokenxxx', 'geh', 'lieblingsserie', 'schau', ':d', '!', 'norma']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_6C-FaR5k5"
      },
      "source": [
        "# Train Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_F5GiXBIRgxI"
      },
      "outputs": [],
      "source": [
        "#training 75%, test 25%\n",
        "SemEval_train = SemEval.iloc[:9213]\n",
        "SemEval_test = SemEval.iloc[9214:]\n",
        "\n",
        "\n",
        "DAI_train = DAI.iloc[:1336]\n",
        "DAI_test = DAI.iloc[1337:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_qNQn6D-edc8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # For clarity:  function to convert string sentiment to lower, or int to lower string\n",
        "# def format(df):\n",
        "#   # labels = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
        "#   labels = {\"Negative\": 'negative', \"Neutral\" : 'neutral', \"Positive\" : 'positive'}\n",
        "#   df['Sentiment'] = df['Sentiment'].map(labels)\n",
        "#   return df[[\"Sentiment\", \"Tweet\"]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZwCIOvi-dPY"
      },
      "source": [
        "# SKLEARN MODELS\n",
        "\n",
        "Preperation for easier model input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H0X4cs7R-cpu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# NOTE: Needed to assign an empty preprocess and tokenizer lamba function, else fit-transforming wouldn't work for some reason. I still don't know why.\n",
        "tfidf_SemEval = TfidfVectorizer(max_features=2000, ngram_range=(1,1), lowercase=False,preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
        "tfidf_DAI = TfidfVectorizer(max_features=2000, ngram_range=(1,1), lowercase=False, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
        "\n",
        "\n",
        "classification_labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "\n",
        "# tfidf_SemEval = CountVectorizer(max_features=2000, ngram_range=(1,2), lowercase=False,preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
        "# tfidf_DAI = CountVectorizer(max_features=2000, ngram_range=(1,2), lowercase=False,preprocessor=lambda x: x, tokenizer=lambda x: x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zmjJ9KXr-1DI"
      },
      "outputs": [],
      "source": [
        "#SemEval\n",
        "SemEval_train_tweet = tfidf_SemEval.fit_transform(SemEval_train['clean'])\n",
        "SemEval_train_label = SemEval_train['Sentiment']\n",
        "\n",
        "SemEval_test_tweet = tfidf_SemEval.transform(SemEval_test['clean'])\n",
        "SemEval_test_label = SemEval_test['Sentiment']\n",
        "\n",
        "#DAI\n",
        "DAI_train_tweet = tfidf_DAI.fit_transform(DAI_train['clean'])\n",
        "DAI_train_label = DAI_train[\"Sentiment\"]\n",
        "\n",
        "DAI_test_tweet = tfidf_DAI.transform(DAI_test['clean'])\n",
        "DAI_test_label = DAI_test[\"Sentiment\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4PSzxZ5VpJJU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#German labelled and English translation\n",
        "German_tweet = tfidf_DAI.transform(German['clean'])\n",
        "German_label = German[\"Sentiment\"]\n",
        "\n",
        "German_translated_tweet = tfidf_SemEval.transform(German_translated['clean'])\n",
        "German_translated_label = German_translated[\"Sentiment\"]\n",
        "\n",
        "#English labelled and German translation\n",
        "English_tweet = tfidf_SemEval.transform(English['clean'])\n",
        "English_label = English[\"Sentiment\"]\n",
        "\n",
        "English_translated_tweet = tfidf_DAI.transform(English_translated['clean'])\n",
        "English_translated_label = English_translated[\"Sentiment\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "odbritwStmQz",
        "outputId": "30791c17-5a82-4c08-bac3-17ae8df54111"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-241e4263-1ed1-4e97-86a1-222f7f347261\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Chloe</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>@Marcel126610 Irgendwie schon. Aber ...wer will ihn denn dann auch irgendwo rumsitzen haben...ü§∑‚Äç‚ôÄÔ∏è</td>\n",
              "      <td>[xxxusernamexxx, irgendwie, schon, wer, irgendwo, rumsitz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>@PewPeeew aight nichts leichter als das</td>\n",
              "      <td>[xxxusernamexxx, aigh, nich, leich]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>@LibertyHannes @RikeWaldfee @MarcoBuschmann Uhhhhh echt jetzt \" ich darf t√∂ten wenn ich will\" mimimi üò≠ und was mit euren Blagen ist, ist mir doch egal. Puh was ich dazu sage #FDPmachtkrankundarm  und #FDPunter5Prozent . Ihr seit auf einen guten Weg . üòÇ Ach #dielinke erholt sich gerade in den ersten Umfragen wider. üòÇ</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, uhhhhh, ech, darf, tot, mimimi, blag, egal, puh, sag, fdpmachtkrankundarm, fdpu, 5proz, seit, gut, ach, dielink, erhol, rad, ers, umfrag, wider]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>Unter #IhrHabtEuchSelbstAusgegrenzt meinte gerade ein Querdenker,  es sollte doch konsequenzlos m√∂glich sein sich nicht impfen zu lassen.   Tja was soll ich sagen: F√ºr Pfleger und √Ñrzte am Ende ihrer Kr√§fte war es auch nicht konsequenzlos, wenn Querdenker die Impfung verweigern.</td>\n",
              "      <td>[ihrhabteuchselbstausgegrenz, mein, rad, querdenk, konsequenzlo, moglich, nich, impf, lass, tja, sag, pfleg, arz, end, kraf, nich, konsequenzlo, querdenk, impfung, verweig]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Und zur Sicherheit noch einmal: es geht hier um die Etikettierung, nicht das Testprodukt selbst. Kein Einfluss auf Ergebnis. https://t.co/Nne9nYSPgV</td>\n",
              "      <td>[sicherhei, geh, etikettierung, nich, testproduk, einfluss, ergebni, xxxurlxxx]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>@MarcBrup @broennimann Welches Land findest Du inspirierend? (Ehrlich gemeinte Frage) Mich inspirieren Menschen, Natur, Musik, etc..aber ein ganzes Land?ü§î pS: ich bin sehr viel gereist, und k√∂nnte h√∂chstens sagen, dass die unterschiedlichsten Menschen und Kulturen inspirierend waren. Die Vielfalt.</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, land, find, inspirier, ehrlich, mein, frag, inspirier, mensch, natur, musik, etc, ganz, land, ps, reis, hoch, sag, unterschiedlich, mensch, kultur, inspirier, vielfal]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>@besserossi @deprecatedCode @kotzlpotzl @drlisamaria Leichenfledderei! üò†</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, leichenfledderei]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>Ihr wurdet nie ausgegrenzt, ihr habt euch separiert  Ihr habt auf die Solidargemeinschaft geschissen  Ihr habt euch von Rechtsextremisten unterst√ºtzen lassen  Ihr werdet in Geschichtsb√ºchern nicht als Opfer sondern als T√§ter stehen  #wirhabenausgegrenzt #SolidaritaetmitderWoelfin</td>\n",
              "      <td>[wurd, nie, ausgegrenz, hab, separie, hab, solidargemeinschaf, schiss, hab, rechtsextremi, unterstutz, lass, werd, schichtsbuch, nich, opfer, tater, steh, wirhabenausgegrenz, solidaritaetmitderwoelfi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>Der Anwalt Chan-jo Jun hat Twitter verlassen. St√§ndig forderte er, dass wir mehr tun im Bereich Hasskriminalit√§t u. bekam sehr viele Morddrohungen. Verstehe, dass er jetzt genug hat. Tragisch ist sein R√ºckzug hier f√ºr unsere Demokratie dennoch. Hoffe sehr, er kommt zur√ºck.</td>\n",
              "      <td>[anwal, cha, jo, jun, twitt, verlass, standig, ford, mehr, tun, bereich, hasskriminalita, bekam, viel, morddrohung, versteh, genug, tragisch, ruckzug, demokratie, dennoch, hoff, komm, zuruck]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>G√ºnstiges #Russland-#Gas ist das beste Mittel gegen Verm√∂gensfra√ü beim B√ºrger. Stattdessen kommen #Gruene mit #Gasumlage und anderen #Grunen Plagen.</td>\n",
              "      <td>[gunstig, russla, gas, bes, mittel, vermogensfrass, beim, burg, stattdess, komm, gru, gasumlag, gru, plag]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>@kaktuskoeln @J_Todenhoefer Vielleicht kriege ich diese seltsamen Leute auch nur durcheinander.  Kann man sich ja nicht alle merken.</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, vielleich, krieg, seltsam, leu, durcheina, ja, nich, merk]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>@moraimauy @ViktoriaGabor38 Welches Ort?Schreiben Sie bitte.</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, oschreib, bitt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>@DerHeuwer Ich hatte Pommes Spezial Spezial. Mayo, Hollandaise und Rostzwiebeln. Ich bin sehr gl√ºcklich ‚ò∫Ô∏è https://t.co/Ukx5UUuk1q</td>\n",
              "      <td>[xxxusernamexxx, pomm, spezial, spezial, mayo, hollandai, rostzwiebel, glucklich, xxxurlxxx]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-241e4263-1ed1-4e97-86a1-222f7f347261')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-241e4263-1ed1-4e97-86a1-222f7f347261 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-241e4263-1ed1-4e97-86a1-222f7f347261');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Sentiment  Chloe  \\\n",
              "0           0   -1.0   \n",
              "1           0    1.0   \n",
              "2          -1   -1.0   \n",
              "3           0   -1.0   \n",
              "4           0    0.0   \n",
              "5           1    1.0   \n",
              "6          -1   -1.0   \n",
              "7          -1   -1.0   \n",
              "8          -1   -1.0   \n",
              "9           0    0.0   \n",
              "10          0    0.0   \n",
              "11          0    0.0   \n",
              "12          1    1.0   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                            Tweet  \\\n",
              "0                                                                                                                                                                                                                              @Marcel126610 Irgendwie schon. Aber ...wer will ihn denn dann auch irgendwo rumsitzen haben...ü§∑‚Äç‚ôÄÔ∏è   \n",
              "1                                                                                                                                                                                                                                                                                         @PewPeeew aight nichts leichter als das   \n",
              "2   @LibertyHannes @RikeWaldfee @MarcoBuschmann Uhhhhh echt jetzt \" ich darf t√∂ten wenn ich will\" mimimi üò≠ und was mit euren Blagen ist, ist mir doch egal. Puh was ich dazu sage #FDPmachtkrankundarm  und #FDPunter5Prozent . Ihr seit auf einen guten Weg . üòÇ Ach #dielinke erholt sich gerade in den ersten Umfragen wider. üòÇ   \n",
              "3                                         Unter #IhrHabtEuchSelbstAusgegrenzt meinte gerade ein Querdenker,  es sollte doch konsequenzlos m√∂glich sein sich nicht impfen zu lassen.   Tja was soll ich sagen: F√ºr Pfleger und √Ñrzte am Ende ihrer Kr√§fte war es auch nicht konsequenzlos, wenn Querdenker die Impfung verweigern.   \n",
              "4                                                                                                                                                                            Und zur Sicherheit noch einmal: es geht hier um die Etikettierung, nicht das Testprodukt selbst. Kein Einfluss auf Ergebnis. https://t.co/Nne9nYSPgV   \n",
              "5                      @MarcBrup @broennimann Welches Land findest Du inspirierend? (Ehrlich gemeinte Frage) Mich inspirieren Menschen, Natur, Musik, etc..aber ein ganzes Land?ü§î pS: ich bin sehr viel gereist, und k√∂nnte h√∂chstens sagen, dass die unterschiedlichsten Menschen und Kulturen inspirierend waren. Die Vielfalt.   \n",
              "6                                                                                                                                                                                                                                                        @besserossi @deprecatedCode @kotzlpotzl @drlisamaria Leichenfledderei! üò†   \n",
              "7                                        Ihr wurdet nie ausgegrenzt, ihr habt euch separiert  Ihr habt auf die Solidargemeinschaft geschissen  Ihr habt euch von Rechtsextremisten unterst√ºtzen lassen  Ihr werdet in Geschichtsb√ºchern nicht als Opfer sondern als T√§ter stehen  #wirhabenausgegrenzt #SolidaritaetmitderWoelfin   \n",
              "8                                               Der Anwalt Chan-jo Jun hat Twitter verlassen. St√§ndig forderte er, dass wir mehr tun im Bereich Hasskriminalit√§t u. bekam sehr viele Morddrohungen. Verstehe, dass er jetzt genug hat. Tragisch ist sein R√ºckzug hier f√ºr unsere Demokratie dennoch. Hoffe sehr, er kommt zur√ºck.   \n",
              "9                                                                                                                                                                            G√ºnstiges #Russland-#Gas ist das beste Mittel gegen Verm√∂gensfra√ü beim B√ºrger. Stattdessen kommen #Gruene mit #Gasumlage und anderen #Grunen Plagen.   \n",
              "10                                                                                                                                                                                           @kaktuskoeln @J_Todenhoefer Vielleicht kriege ich diese seltsamen Leute auch nur durcheinander.  Kann man sich ja nicht alle merken.   \n",
              "11                                                                                                                                                                                                                                                                   @moraimauy @ViktoriaGabor38 Welches Ort?Schreiben Sie bitte.   \n",
              "12                                                                                                                                                                                             @DerHeuwer Ich hatte Pommes Spezial Spezial. Mayo, Hollandaise und Rostzwiebeln. Ich bin sehr gl√ºcklich ‚ò∫Ô∏è https://t.co/Ukx5UUuk1q   \n",
              "\n",
              "                                                                                                                                                                                                       clean  \n",
              "0                                                                                                                                                 [xxxusernamexxx, irgendwie, schon, wer, irgendwo, rumsitz]  \n",
              "1                                                                                                                                                                        [xxxusernamexxx, aigh, nich, leich]  \n",
              "2          [xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, uhhhhh, ech, darf, tot, mimimi, blag, egal, puh, sag, fdpmachtkrankundarm, fdpu, 5proz, seit, gut, ach, dielink, erhol, rad, ers, umfrag, wider]  \n",
              "3                               [ihrhabteuchselbstausgegrenz, mein, rad, querdenk, konsequenzlo, moglich, nich, impf, lass, tja, sag, pfleg, arz, end, kraf, nich, konsequenzlo, querdenk, impfung, verweig]  \n",
              "4                                                                                                                            [sicherhei, geh, etikettierung, nich, testproduk, einfluss, ergebni, xxxurlxxx]  \n",
              "5   [xxxusernamexxx, xxxusernamexxx, land, find, inspirier, ehrlich, mein, frag, inspirier, mensch, natur, musik, etc, ganz, land, ps, reis, hoch, sag, unterschiedlich, mensch, kultur, inspirier, vielfal]  \n",
              "6                                                                                                                         [xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, leichenfledderei]  \n",
              "7   [wurd, nie, ausgegrenz, hab, separie, hab, solidargemeinschaf, schiss, hab, rechtsextremi, unterstutz, lass, werd, schichtsbuch, nich, opfer, tater, steh, wirhabenausgegrenz, solidaritaetmitderwoelfi]  \n",
              "8            [anwal, cha, jo, jun, twitt, verlass, standig, ford, mehr, tun, bereich, hasskriminalita, bekam, viel, morddrohung, versteh, genug, tragisch, ruckzug, demokratie, dennoch, hoff, komm, zuruck]  \n",
              "9                                                                                                 [gunstig, russla, gas, bes, mittel, vermogensfrass, beim, burg, stattdess, komm, gru, gasumlag, gru, plag]  \n",
              "10                                                                                                               [xxxusernamexxx, xxxusernamexxx, vielleich, krieg, seltsam, leu, durcheina, ja, nich, merk]  \n",
              "11                                                                                                                                                          [xxxusernamexxx, xxxusernamexxx, oschreib, bitt]  \n",
              "12                                                                                                              [xxxusernamexxx, pomm, spezial, spezial, mayo, hollandai, rostzwiebel, glucklich, xxxurlxxx]  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "German.head(13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aOh9SydLtECs",
        "outputId": "cd7f2f91-0e6e-4c57-bcaf-6f0088a338ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-66f25d44-0104-4134-9b9c-73e8ef751e6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@Marcel126610 Somehow yes. But...who wants it sitting around somewhere...ü§∑‚Äç‚ôÄÔ∏è</td>\n",
              "      <td>[xxxusernamexxx, somehow, ye, want, sit, around, somewher]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@PewPeeew aight nothing easier than that</td>\n",
              "      <td>[xxxusernamexxx, aight, noth, easier]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>@LibertyHannes @RikeWaldfee @MarcoBuschmann Uhhhhh really now \"I can kill if I want\" mimimi üò≠ and I don't care what's up with your brats. Puh what I say to #FDPmachtsickundarm and #FDPunter5Percent. You are on the right track. üòÇ Oh #dielinke is recovering in the first polls. üòÇ</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, uhhhhh, realli, kill, want, mimimi, care, brat, puh, say, fdpmachtsickundarm, fdpunter, 5percent, right, track, oh, dielink, recov, first, poll]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Under #YourHaveYouselfExcluded, a lateral thinker said that it should be possible not to be vaccinated without any consequences. Well, what can I say: For nurses and doctors at the end of their strength, it was not without consequences when lateral thinkers refused to be vaccinated.</td>\n",
              "      <td>[yourhaveyouselfexclud, later, thinker, said, possibl, not, vaccin, without, consequ, well, say, nurs, doctor, end, strength, not, without, consequ, later, thinker, refus, vaccin]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>And to be on the safe side again: this is about the labeling, not the test product itself. No influence on the result. https://t.co/Nne9nYSPgV</td>\n",
              "      <td>[safe, side, label, not, test, product, influenc, result, xxxurlxxx]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>@MarcBrup @broennimann Which country do you find inspirational? (Honest question) I'm inspired by people, nature, music, etc.. but a whole country?ü§î PS: I've traveled a lot and could at most say that the most diverse people and cultures were inspiring. The diversity.</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, countri, find, inspir, honest, question, inspir, peopl, natur, music, etc, whole, countri, ps, travel, lot, could, say, divers, peopl, cultur, inspir, divers]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>@besserossi @deprecatedCode @kotzlpotzl @drlisamaria scavenging! üò†</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, scaveng]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>You were never excluded, you separated. You gave a shit about the community of solidarity. You let right-wing extremists support you. You will not appear in history books as a victim but as a perpetrator #we have excluded #SolidaritaetmitderWoelfin</td>\n",
              "      <td>[never, exclud, separ, gave, shit, commun, solidar, let, right, wing, extremist, suppoy, not, appear, histori, book, victim, perpetr, exclud, solidaritaetmitderwoelfin]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1</td>\n",
              "      <td>Lawyer Chan-jo Jun has left Twitter. He kept demanding that we do more on hate crime and received many death threats. Understand that he's had enough now. His withdrawal here is still tragic for our democracy. I really hope he comes back.</td>\n",
              "      <td>[lawyer, chan, jo, jun, left, twitter, kept, demand, hate, crime, receiv, mani, death, threat, understand, enough, withdraw, still, tragic, democraci, realli, hope, come, back]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>Cheap #Russia #gas is the best remedy against the citizens' wealth being eaten up. Instead, #greens come with #gas surcharges and other #green plagues.</td>\n",
              "      <td>[cheap, russia, ga, best, remedi, citizen, wealth, eaten, instead, green, come, ga, surcharg, green, plagu]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>@kaktuskoeln @J_Todenhoefer Maybe I'm just confusing these strange people. You can't remember them all.</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, mayb, confus, strang, peopl, rememb]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>@moraimauy @ViktoriaGabor38 Which place? Write please.</td>\n",
              "      <td>[xxxusernamexxx, xxxusernamexxx, place, write, pleas]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>@DerHeuwer I had fries special special. Mayo, hollandaise and fried onions. I am very happy ‚ò∫Ô∏è https://t.co/Ukx5UUuk1q</td>\n",
              "      <td>[xxxusernamexxx, fri, special, special, mayo, hollandais, fri, onion, happi, xxxurlxxx]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66f25d44-0104-4134-9b9c-73e8ef751e6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-66f25d44-0104-4134-9b9c-73e8ef751e6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-66f25d44-0104-4134-9b9c-73e8ef751e6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Sentiment  \\\n",
              "0           0   \n",
              "1           0   \n",
              "2          -1   \n",
              "3           0   \n",
              "4           0   \n",
              "5           1   \n",
              "6          -1   \n",
              "7          -1   \n",
              "8          -1   \n",
              "9           0   \n",
              "10          0   \n",
              "11          0   \n",
              "12          1   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                          Tweet  \\\n",
              "0                                                                                                                                                                                                                 @Marcel126610 Somehow yes. But...who wants it sitting around somewhere...ü§∑‚Äç‚ôÄÔ∏è   \n",
              "1                                                                                                                                                                                                                                                      @PewPeeew aight nothing easier than that   \n",
              "2         @LibertyHannes @RikeWaldfee @MarcoBuschmann Uhhhhh really now \"I can kill if I want\" mimimi üò≠ and I don't care what's up with your brats. Puh what I say to #FDPmachtsickundarm and #FDPunter5Percent. You are on the right track. üòÇ Oh #dielinke is recovering in the first polls. üòÇ   \n",
              "3   Under #YourHaveYouselfExcluded, a lateral thinker said that it should be possible not to be vaccinated without any consequences. Well, what can I say: For nurses and doctors at the end of their strength, it was not without consequences when lateral thinkers refused to be vaccinated.   \n",
              "4                                                                                                                                                And to be on the safe side again: this is about the labeling, not the test product itself. No influence on the result. https://t.co/Nne9nYSPgV   \n",
              "5                   @MarcBrup @broennimann Which country do you find inspirational? (Honest question) I'm inspired by people, nature, music, etc.. but a whole country?ü§î PS: I've traveled a lot and could at most say that the most diverse people and cultures were inspiring. The diversity.   \n",
              "6                                                                                                                                                                                                                            @besserossi @deprecatedCode @kotzlpotzl @drlisamaria scavenging! üò†   \n",
              "7                                      You were never excluded, you separated. You gave a shit about the community of solidarity. You let right-wing extremists support you. You will not appear in history books as a victim but as a perpetrator #we have excluded #SolidaritaetmitderWoelfin   \n",
              "8                                                Lawyer Chan-jo Jun has left Twitter. He kept demanding that we do more on hate crime and received many death threats. Understand that he's had enough now. His withdrawal here is still tragic for our democracy. I really hope he comes back.   \n",
              "9                                                                                                                                       Cheap #Russia #gas is the best remedy against the citizens' wealth being eaten up. Instead, #greens come with #gas surcharges and other #green plagues.   \n",
              "10                                                                                                                                                                                      @kaktuskoeln @J_Todenhoefer Maybe I'm just confusing these strange people. You can't remember them all.   \n",
              "11                                                                                                                                                                                                                                       @moraimauy @ViktoriaGabor38 Which place? Write please.   \n",
              "12                                                                                                                                                                       @DerHeuwer I had fries special special. Mayo, hollandaise and fried onions. I am very happy ‚ò∫Ô∏è https://t.co/Ukx5UUuk1q   \n",
              "\n",
              "                                                                                                                                                                                                clean  \n",
              "0                                                                                                                                          [xxxusernamexxx, somehow, ye, want, sit, around, somewher]  \n",
              "1                                                                                                                                                               [xxxusernamexxx, aight, noth, easier]  \n",
              "2   [xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, uhhhhh, realli, kill, want, mimimi, care, brat, puh, say, fdpmachtsickundarm, fdpunter, 5percent, right, track, oh, dielink, recov, first, poll]  \n",
              "3                 [yourhaveyouselfexclud, later, thinker, said, possibl, not, vaccin, without, consequ, well, say, nurs, doctor, end, strength, not, without, consequ, later, thinker, refus, vaccin]  \n",
              "4                                                                                                                                [safe, side, label, not, test, product, influenc, result, xxxurlxxx]  \n",
              "5     [xxxusernamexxx, xxxusernamexxx, countri, find, inspir, honest, question, inspir, peopl, natur, music, etc, whole, countri, ps, travel, lot, could, say, divers, peopl, cultur, inspir, divers]  \n",
              "6                                                                                                                           [xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, xxxusernamexxx, scaveng]  \n",
              "7                            [never, exclud, separ, gave, shit, commun, solidar, let, right, wing, extremist, suppoy, not, appear, histori, book, victim, perpetr, exclud, solidaritaetmitderwoelfin]  \n",
              "8                    [lawyer, chan, jo, jun, left, twitter, kept, demand, hate, crime, receiv, mani, death, threat, understand, enough, withdraw, still, tragic, democraci, realli, hope, come, back]  \n",
              "9                                                                                         [cheap, russia, ga, best, remedi, citizen, wealth, eaten, instead, green, come, ga, surcharg, green, plagu]  \n",
              "10                                                                                                                              [xxxusernamexxx, xxxusernamexxx, mayb, confus, strang, peopl, rememb]  \n",
              "11                                                                                                                                              [xxxusernamexxx, xxxusernamexxx, place, write, pleas]  \n",
              "12                                                                                                            [xxxusernamexxx, fri, special, special, mayo, hollandais, fri, onion, happi, xxxurlxxx]  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "German_translated.head(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ======= MODEL TRAINING AND PREDICTION ======="
      ],
      "metadata": {
        "id": "SWGLHmWSd30j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPFG6W-YZdN0"
      },
      "source": [
        "# Multinomial NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JaSHF73gZhSH",
        "outputId": "aea19e82-be95-4acf-838d-afb7c5472ca5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#SemEval alone\n",
        "nb_SemEval = MultinomialNB()\n",
        "nb_SemEval.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_MNB_SemEval = nb_SemEval.predict(SemEval_test_tweet)\n",
        "MNB_SemEval = classification_report(SemEval_test_label, pred_MNB_SemEval, target_names= classification_labels)\n",
        "\n",
        "#SemEval on English labelled\n",
        "nb_SemEval_english = MultinomialNB()\n",
        "nb_SemEval_english.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_MNB_SemEval_english = nb_SemEval_english.predict(English_tweet)\n",
        "MNB_SemEval_ENGLISH = classification_report(English_label, pred_MNB_SemEval_english, target_names= classification_labels)\n",
        "\n",
        "#SemEval on German labelled tranlsated into English\n",
        "nb_SemEval_german = MultinomialNB()\n",
        "nb_SemEval_german.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_MNB_SemEval_german = nb_SemEval_german.predict(German_translated_tweet)\n",
        "MNB_SemEval_GERMAN = classification_report(German_translated_label, pred_MNB_SemEval_german, target_names= classification_labels)\n",
        "\n",
        "#German_Unqiue alone\n",
        "nb_DAI = MultinomialNB()\n",
        "nb_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_MNB_DAI = nb_DAI.predict(DAI_test_tweet)\n",
        "MNB_DAI = classification_report(DAI_test_label, pred_MNB_DAI, target_names= classification_labels)\n",
        "\n",
        "\n",
        "#German_Unqiue on German\n",
        "nb_DAI_german = MultinomialNB()\n",
        "nb_DAI_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_MNB_DAI_german = nb_DAI_german.predict(German_tweet)\n",
        "MNB_DAI_GERMAN = classification_report(German_label, pred_MNB_DAI_german, target_names= classification_labels)\n",
        "\n",
        "\n",
        "#German_Unqiue on English labelled transalted into German\n",
        "nb_german = MultinomialNB()\n",
        "nb_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_MNB_German = nb_german.predict(English_translated_tweet)\n",
        "MNB_DAI_ENGLISH  = classification_report(English_translated_label, pred_MNB_German, target_names= classification_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q46EnswxCF9H"
      },
      "source": [
        "# Logistic Regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JPP77BdDCc8L",
        "outputId": "303d96bd-07bd-4ce4-eebe-56bd55207352"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "#SemEval alone\n",
        "LRmodel_SemEval = LogisticRegression( max_iter = 100)\n",
        "LRmodel_SemEval.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_LR_SemEval = LRmodel_SemEval.predict(SemEval_test_tweet)\n",
        "LOGREG_SemEval = classification_report(SemEval_test_label, pred_LR_SemEval, target_names= classification_labels)\n",
        "\n",
        "\n",
        "#SemEval on English labelled\n",
        "LRmodel_SemEval_english = LogisticRegression(max_iter = 100)\n",
        "LRmodel_SemEval_english.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_LR_SemEval_english = LRmodel_SemEval_english.predict(English_tweet)\n",
        "LOGREG_SemEval_ENGLISH = classification_report(English_label, pred_LR_SemEval_english, target_names= classification_labels)\n",
        "\n",
        "#SemEval on German labelled tranlsated into English\n",
        "LRmodel_SemEval_german = LogisticRegression(max_iter = 100,)\n",
        "LRmodel_SemEval_german.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_LR_SemEval_german = LRmodel_SemEval_german.predict(German_translated_tweet)\n",
        "LOGREG_SemEval_GERMAN = classification_report(German_translated_label, pred_LR_SemEval_german, target_names= classification_labels)\n",
        "\n",
        "#DAI alone\n",
        "LRmodel_DAI = LogisticRegression(max_iter = 100)\n",
        "LRmodel_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_LR_DAI = LRmodel_DAI.predict(DAI_test_tweet)\n",
        "LOGREG_DAI = classification_report(DAI_test_label, pred_LR_DAI, target_names= classification_labels)\n",
        "\n",
        "#German_Unqiue on German\n",
        "LRmodel_DAI_german = LogisticRegression(max_iter = 100)\n",
        "LRmodel_DAI_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_LR_DAI_german = LRmodel_DAI_german.predict(German_tweet)\n",
        "LOGREG_DAI_GERMAN = classification_report(German_label, pred_LR_DAI_german, target_names= classification_labels)\n",
        "\n",
        "\n",
        "#German_Unqiue on English labelled transalted into German\n",
        "LRmodel_german = LogisticRegression(max_iter = 100)\n",
        "LRmodel_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_LR_german = LRmodel_german.predict(English_translated_tweet)\n",
        "LOGREG_DAI_ENGLISH  = classification_report(English_translated_label, pred_LR_german, target_names= classification_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryj5VmpyCFvY"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynSFZoJaCjMt"
      },
      "outputs": [],
      "source": [
        "#SemEval alone\n",
        "svcl_SemEval = svm.SVC()\n",
        "svcl_SemEval.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_SVC_SemEval = svcl_SemEval.predict(SemEval_test_tweet)\n",
        "SVM_SemEval = classification_report(SemEval_test_label, pred_SVC_SemEval, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "#SemEval on English labelled\n",
        "svcl_SemEval_english = svm.SVC()\n",
        "svcl_SemEval_english.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_SVC_SemEval_english = svcl_SemEval_english.predict(English_tweet)\n",
        "SVM_SemEval_ENGLISH = classification_report(English_label, pred_SVC_SemEval_english, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "\n",
        "#SemEval on German labelled tranlsated into English\n",
        "svcl_SemEval_german = svm.SVC()\n",
        "svcl_SemEval_german.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_SVC_SemEval_german = svcl_SemEval_german.predict(German_translated_tweet)\n",
        "SVM_SemEval_GERMAN= classification_report(German_translated_label, pred_SVC_SemEval_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "#DAI alone\n",
        "svcl_DAI= svm.SVC()\n",
        "svcl_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_SVC_DAI= svcl_DAI.predict(DAI_test_tweet)\n",
        "SVM_DAI= classification_report(DAI_test_label, pred_SVC_DAI, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "#DAI on German\n",
        "svcl_DAI_german = svm.SVC()\n",
        "svcl_DAI_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_SVC_DAI_german = svcl_DAI_german.predict(German_tweet)\n",
        "SVM_DAI_GERMAN = classification_report(German_label, pred_SVC_DAI_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "\n",
        "#DAI on English labelled transalted into German\n",
        "svcl_german = svm.SVC()\n",
        "svcl_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_SVC_german = svcl_german.predict(English_translated_tweet)\n",
        "SVM_DAI_ENGLISH = classification_report(English_translated_label, pred_SVC_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auwwH2t9CFcp"
      },
      "source": [
        "#K Nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xaydAlRWCoKo"
      },
      "outputs": [],
      "source": [
        "#SemEval alone\n",
        "knn_SemEval = KNeighborsClassifier(n_neighbors=4)\n",
        "knn_SemEval.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_knn_SemEval = knn_SemEval.predict(SemEval_test_tweet)\n",
        "KNN_SemEval = classification_report(SemEval_test_label, pred_knn_SemEval, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "#SemEval on English labelled\n",
        "knn_SemEval_english = KNeighborsClassifier(n_neighbors=4)\n",
        "knn_SemEval_english.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_knn_SemEval_english = knn_SemEval_english.predict(English_tweet)\n",
        "KNN_SemEval_ENGLISH = classification_report(English_label, pred_knn_SemEval_english, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "#SemEval on German labelled tranlsated into English\n",
        "knn_SemEval_german = KNeighborsClassifier(n_neighbors=4)\n",
        "knn_SemEval_german.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "pred_knn_SemEval_german = knn_SemEval_german.predict(German_translated_tweet)\n",
        "KNN_SemEval_GERMAN= classification_report(German_translated_label, pred_knn_SemEval_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "#DAI alone\n",
        "knn_DAI = KNeighborsClassifier(n_neighbors=4)\n",
        "knn_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_knn_DAI = knn_DAI.predict(DAI_test_tweet)\n",
        "KNN_DAI = classification_report(DAI_test_label, pred_knn_DAI, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "#DAI on German\n",
        "knn_DAI_german = KNeighborsClassifier(n_neighbors=4)\n",
        "knn_DAI_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_knn_DAI_german = knn_DAI_german.predict(German_tweet)\n",
        "KNN_DAI_GERMAN = classification_report(German_label, pred_knn_DAI_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "#DAI on English labelled transalted into German\n",
        "knn_german = KNeighborsClassifier(n_neighbors=4)\n",
        "knn_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "pred_knn_german = knn_german.predict(English_translated_tweet)\n",
        "KNN_DAI_ENGLISH = classification_report(English_translated_label, pred_knn_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c246p18C1g3"
      },
      "source": [
        "# SEMEVAL ALONE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dsAgynSHv6xU",
        "outputId": "adb61712-4d04-4406-d86c-aa4b58a87179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================== RESULTS FOR SemEval on itself ==================\n",
            "\n",
            "=====Support Vector Machine_SemEval=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.68      0.53      0.59      1010\n",
            "     Neutral       0.60      0.80      0.69      1476\n",
            "    Positive       0.71      0.39      0.50       584\n",
            "\n",
            "    accuracy                           0.63      3070\n",
            "   macro avg       0.67      0.57      0.60      3070\n",
            "weighted avg       0.65      0.63      0.62      3070\n",
            "\n",
            "=====Multinominal NB_SemEval=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.67      0.56      0.61      1010\n",
            "     Neutral       0.61      0.77      0.68      1476\n",
            "    Positive       0.64      0.39      0.48       584\n",
            "\n",
            "    accuracy                           0.63      3070\n",
            "   macro avg       0.64      0.57      0.59      3070\n",
            "weighted avg       0.63      0.63      0.62      3070\n",
            "\n",
            "=====Logistic Regression_SemEval=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.66      0.57      0.61      1010\n",
            "     Neutral       0.62      0.75      0.68      1476\n",
            "    Positive       0.65      0.45      0.53       584\n",
            "\n",
            "    accuracy                           0.63      3070\n",
            "   macro avg       0.64      0.59      0.61      3070\n",
            "weighted avg       0.64      0.63      0.63      3070\n",
            "\n",
            "=====K Nearest Neighbor_SemEval=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.47      0.10      0.16      1010\n",
            "     Neutral       0.49      0.85      0.62      1476\n",
            "    Positive       0.37      0.18      0.25       584\n",
            "\n",
            "    accuracy                           0.48      3070\n",
            "   macro avg       0.44      0.38      0.34      3070\n",
            "weighted avg       0.46      0.48      0.40      3070\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"====================== RESULTS FOR SemEval on itself ==================\\n\")\n",
        "\n",
        "print(\"=====Support Vector Machine_SemEval=====\\n\")\n",
        "print(SVM_SemEval)\n",
        "\n",
        "print(\"=====Multinominal NB_SemEval=====\\n\")\n",
        "print(MNB_SemEval)\n",
        "\n",
        "print(\"=====Logistic Regression_SemEval=====\\n\")\n",
        "print(LOGREG_SemEval)\n",
        "\n",
        "print(\"=====K Nearest Neighbor_SemEval=====\\n\")\n",
        "print(KNN_SemEval)\n",
        "\n",
        "# print(\"===== SGD_SemEval=====\\n\")\n",
        "# print(SGD_SemEval)\n",
        "\n",
        "# print(\"=====LINEAR SVC_SemEval=====\\n\")\n",
        "# print(LINEARSVC_SemEval)\n",
        "\n",
        "# print(\"=====Bernoulli NB_SemEval=====\\n\")\n",
        "# print(BNB_SemEval)\n",
        "\n",
        "# print(\"=====Random Forest_SemEval=====\\n\")\n",
        "# print(RANFOR_SemEval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kWratcmZC1Z2",
        "outputId": "e32aa3f6-63c4-463f-f5f9-71f7a8c11ece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================== RESULTS FOR SemEval ON ENGLISH DATASET ==================\n",
            "\n",
            "=====Support Vector Machine_SemEval_ENGLISH=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.37      0.39      0.38       132\n",
            "     Neutral       0.77      0.84      0.80       679\n",
            "    Positive       0.63      0.39      0.48       167\n",
            "\n",
            "    accuracy                           0.70       978\n",
            "   macro avg       0.59      0.54      0.56       978\n",
            "weighted avg       0.70      0.70      0.69       978\n",
            "\n",
            "=====Multinominal NB_SemEval_ENGLISH=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.35      0.44      0.39       132\n",
            "     Neutral       0.77      0.82      0.79       679\n",
            "    Positive       0.63      0.31      0.42       167\n",
            "\n",
            "    accuracy                           0.68       978\n",
            "   macro avg       0.58      0.52      0.53       978\n",
            "weighted avg       0.69      0.68      0.67       978\n",
            "\n",
            "=====Logistic Regression_SemEval_ENGLISH=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.36      0.45      0.40       132\n",
            "     Neutral       0.79      0.80      0.79       679\n",
            "    Positive       0.59      0.46      0.51       167\n",
            "\n",
            "    accuracy                           0.69       978\n",
            "   macro avg       0.58      0.57      0.57       978\n",
            "weighted avg       0.70      0.69      0.69       978\n",
            "\n",
            "=====K Nearest Neighbor_SemEval_ENGLISH=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.11      0.05      0.07       132\n",
            "     Neutral       0.70      0.80      0.75       679\n",
            "    Positive       0.23      0.19      0.21       167\n",
            "\n",
            "    accuracy                           0.59       978\n",
            "   macro avg       0.35      0.35      0.34       978\n",
            "weighted avg       0.54      0.59      0.56       978\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"====================== RESULTS FOR SemEval ON ENGLISH DATASET ==================\\n\")\n",
        "\n",
        "print(\"=====Support Vector Machine_SemEval_ENGLISH=====\\n\")\n",
        "print(SVM_SemEval_ENGLISH)\n",
        "\n",
        "print(\"=====Multinominal NB_SemEval_ENGLISH=====\\n\")\n",
        "print(MNB_SemEval_ENGLISH)\n",
        "\n",
        "print(\"=====Logistic Regression_SemEval_ENGLISH=====\\n\")\n",
        "print(LOGREG_SemEval_ENGLISH)\n",
        "\n",
        "print(\"=====K Nearest Neighbor_SemEval_ENGLISH=====\\n\")\n",
        "print(KNN_SemEval_ENGLISH)\n",
        "\n",
        "# print(\"===== SGD_SemEval_ENGLISH=====\\n\")\n",
        "# print(SGD_SemEval_ENGLISH)\n",
        "\n",
        "# print(\"=====LINEAR SVC_SemEval_ENGLISH=====\\n\")\n",
        "# print(LINEARSVC_SemEval_ENGLISH)\n",
        "\n",
        "# print(\"=====Bernoulli NB_SemEval_ENGLISH=====\\n\")\n",
        "# print(BNB_SemEval_ENGLISH)\n",
        "\n",
        "# print(\"=====Random Forest_SemEval_ENGLISH=====\\n\")\n",
        "# print(RANFOR_SemEval_ENGLISH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hmCQ93Jb7Jpo",
        "outputId": "bd1bb700-ad0a-4d9a-fd59-94639cdc51f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================== RESULTS FOR SemEval ON GERMAN Translated DATASET ==================\n",
            "\n",
            "=====Support Vector Machine_SemEval_GERMAN=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.36      0.61      0.45       132\n",
            "     Neutral       0.88      0.78      0.83       735\n",
            "    Positive       0.58      0.52      0.55        79\n",
            "\n",
            "    accuracy                           0.73       946\n",
            "   macro avg       0.61      0.63      0.61       946\n",
            "weighted avg       0.78      0.73      0.75       946\n",
            "\n",
            "=====Multinominal NB_SemEval_GERMAN=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.61      0.43       132\n",
            "     Neutral       0.87      0.75      0.81       735\n",
            "    Positive       0.52      0.44      0.48        79\n",
            "\n",
            "    accuracy                           0.71       946\n",
            "   macro avg       0.57      0.60      0.57       946\n",
            "weighted avg       0.76      0.71      0.73       946\n",
            "\n",
            "=====Logistic Regression_SemEval_GERMAN=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.34      0.67      0.45       132\n",
            "     Neutral       0.88      0.72      0.79       735\n",
            "    Positive       0.44      0.52      0.47        79\n",
            "\n",
            "    accuracy                           0.69       946\n",
            "   macro avg       0.55      0.63      0.57       946\n",
            "weighted avg       0.77      0.69      0.72       946\n",
            "\n",
            "=====K Nearest Neighbor_SemEval_GERMAN=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.14      0.05      0.07       132\n",
            "     Neutral       0.78      0.76      0.77       735\n",
            "    Positive       0.15      0.33      0.20        79\n",
            "\n",
            "    accuracy                           0.63       946\n",
            "   macro avg       0.35      0.38      0.35       946\n",
            "weighted avg       0.63      0.63      0.62       946\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"====================== RESULTS FOR SemEval ON GERMAN Translated DATASET ==================\\n\")\n",
        "\n",
        "print(\"=====Support Vector Machine_SemEval_GERMAN=====\\n\")\n",
        "print(SVM_SemEval_GERMAN)\n",
        "\n",
        "print(\"=====Multinominal NB_SemEval_GERMAN=====\\n\")\n",
        "print(MNB_SemEval_GERMAN)\n",
        "\n",
        "print(\"=====Logistic Regression_SemEval_GERMAN=====\\n\")\n",
        "print(LOGREG_SemEval_GERMAN)\n",
        "\n",
        "print(\"=====K Nearest Neighbor_SemEval_GERMAN=====\\n\")\n",
        "print(KNN_SemEval_GERMAN)\n",
        "\n",
        "# print(\"===== SGD_SemEval_GERMAN=====\\n\")\n",
        "# print(SGD_SemEval_GERMAN)\n",
        "\n",
        "# print(\"=====LINEAR SVC_SemEval_GERMAN=====\\n\")\n",
        "# print(LINEARSVC_SemEval_GERMAN)\n",
        "\n",
        "# print(\"=====Bernoulli NB_SemEval_GERMAN=====\\n\")\n",
        "# print(BNB_SemEval_GERMAN)\n",
        "\n",
        "# print(\"=====Random Forest_SemEval_GERMAN=====\\n\")\n",
        "# print(RANFOR_SemEval_GERMAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X67uhmAQmhzE",
        "outputId": "65b07b55-eb5e-4c76-d55a-6ec3c84e6d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================== RESULTS FOR DAI ALONE ==================\n",
            "\n",
            "=====Support Vector Machine_DAI=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.04      0.08        74\n",
            "     Neutral       0.68      0.98      0.80       291\n",
            "    Positive       0.65      0.22      0.32        79\n",
            "\n",
            "    accuracy                           0.68       444\n",
            "   macro avg       0.78      0.41      0.40       444\n",
            "weighted avg       0.73      0.68      0.60       444\n",
            "\n",
            "=====Multinominal NB_DAI=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.01      0.03        74\n",
            "     Neutral       0.67      0.99      0.80       291\n",
            "    Positive       0.69      0.11      0.20        79\n",
            "\n",
            "    accuracy                           0.67       444\n",
            "   macro avg       0.79      0.37      0.34       444\n",
            "weighted avg       0.73      0.67      0.56       444\n",
            "\n",
            "=====Logistic Regression_DAI=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.62      0.07      0.12        74\n",
            "     Neutral       0.69      0.94      0.80       291\n",
            "    Positive       0.56      0.30      0.39        79\n",
            "\n",
            "    accuracy                           0.68       444\n",
            "   macro avg       0.63      0.44      0.44       444\n",
            "weighted avg       0.66      0.68      0.61       444\n",
            "\n",
            "=====K Nearest Neighbor_DAI=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.13      0.11      0.12        74\n",
            "     Neutral       0.67      0.86      0.75       291\n",
            "    Positive       0.38      0.04      0.07        79\n",
            "\n",
            "    accuracy                           0.59       444\n",
            "   macro avg       0.39      0.34      0.31       444\n",
            "weighted avg       0.53      0.59      0.52       444\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"====================== RESULTS FOR DAI ALONE ==================\\n\")\n",
        "\n",
        "print(\"=====Support Vector Machine_DAI=====\\n\")\n",
        "print(SVM_DAI)\n",
        "\n",
        "print(\"=====Multinominal NB_DAI=====\\n\")\n",
        "print(MNB_DAI)\n",
        "\n",
        "print(\"=====Logistic Regression_DAI=====\\n\")\n",
        "print(LOGREG_DAI)\n",
        "\n",
        "print(\"=====K Nearest Neighbor_DAI=====\\n\")\n",
        "print(KNN_DAI)\n",
        "\n",
        "# print(\"===== SGD_DAI=====\\n\")\n",
        "# print(SGD_DAI)\n",
        "\n",
        "# print(\"=====LINEARSVC_DAI=====\\n\")\n",
        "# print(LINEARSVC_DAI)\n",
        "\n",
        "# print(\"=====Bernoulli NB_DAI=====\\n\")\n",
        "# print(BNB_DAI)\n",
        "\n",
        "# print(\"=====Random Forest_DAI=====\\n\")\n",
        "# print(RANFOR_DAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HQKAFK1j7mlq",
        "outputId": "3c1426b7-b126-4771-b01e-d255517b5bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================== RESULTS FOR DAI ON GERMAN ==================\n",
            "\n",
            "=====Support Vector Machine_DAI_GERMAN=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.02      0.04       132\n",
            "     Neutral       0.79      0.99      0.88       735\n",
            "    Positive       0.57      0.10      0.17        79\n",
            "\n",
            "    accuracy                           0.78       946\n",
            "   macro avg       0.70      0.37      0.36       946\n",
            "weighted avg       0.76      0.78      0.70       946\n",
            "\n",
            "=====Multinominal NB_DAI_GERMAN=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00       132\n",
            "     Neutral       0.78      1.00      0.88       735\n",
            "    Positive       1.00      0.04      0.07        79\n",
            "\n",
            "    accuracy                           0.78       946\n",
            "   macro avg       0.59      0.35      0.32       946\n",
            "weighted avg       0.69      0.78      0.69       946\n",
            "\n",
            "=====Logistic Regression_DAI_GERMAN=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.24      0.05      0.09       132\n",
            "     Neutral       0.79      0.93      0.86       735\n",
            "    Positive       0.33      0.20      0.25        79\n",
            "\n",
            "    accuracy                           0.75       946\n",
            "   macro avg       0.46      0.40      0.40       946\n",
            "weighted avg       0.68      0.75      0.70       946\n",
            "\n",
            "=====K Nearest Neighbor_DAI_GERMAN=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.24      0.25      0.24       132\n",
            "     Neutral       0.79      0.86      0.82       735\n",
            "    Positive       0.12      0.01      0.02        79\n",
            "\n",
            "    accuracy                           0.70       946\n",
            "   macro avg       0.38      0.37      0.36       946\n",
            "weighted avg       0.66      0.70      0.68       946\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"====================== RESULTS FOR DAI ON GERMAN ==================\\n\")\n",
        "\n",
        "print(\"=====Support Vector Machine_DAI_GERMAN=====\\n\")\n",
        "print(SVM_DAI_GERMAN)\n",
        "\n",
        "print(\"=====Multinominal NB_DAI_GERMAN=====\\n\")\n",
        "print(MNB_DAI_GERMAN)\n",
        "\n",
        "print(\"=====Logistic Regression_DAI_GERMAN=====\\n\")\n",
        "print(LOGREG_DAI_GERMAN)\n",
        "\n",
        "print(\"=====K Nearest Neighbor_DAI_GERMAN=====\\n\")\n",
        "print(KNN_DAI_GERMAN)\n",
        "\n",
        "# print(\"===== SGD_DAI_GERMAN=====\\n\")\n",
        "# print(SGD_DAI_GERMAN)\n",
        "\n",
        "# print(\"=====LINEAR SVC_DAI_GERMAN=====\\n\")\n",
        "# print(LINEARSVC_DAI_GERMAN)\n",
        "\n",
        "# print(\"=====Bernoulli NB_DAI_GERMANN=====\\n\")\n",
        "# print(BNB_DAI_GERMAN)\n",
        "\n",
        "# print(\"=====Random Forest_DAI_GERMAN=====\\n\")\n",
        "# print(RANFOR_DAI_GERMAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xeXgkkon7-qF",
        "outputId": "d84776d1-eac9-4c8b-e219-ec9b2a2e1e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================== RESULTS FOR DAI ENGLISH TRANSALTED ==================\n",
            "\n",
            "=====Support Vector Machine_DAI_ENGLISH=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.01      0.01       132\n",
            "     Neutral       0.70      0.98      0.82       679\n",
            "    Positive       0.54      0.08      0.15       167\n",
            "\n",
            "    accuracy                           0.70       978\n",
            "   macro avg       0.50      0.36      0.33       978\n",
            "weighted avg       0.61      0.70      0.60       978\n",
            "\n",
            "=====Multinominal NB_UNQIUE_ENGLISH=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00       132\n",
            "     Neutral       0.70      1.00      0.82       679\n",
            "    Positive       0.83      0.03      0.06       167\n",
            "\n",
            "    accuracy                           0.70       978\n",
            "   macro avg       0.51      0.34      0.29       978\n",
            "weighted avg       0.63      0.70      0.58       978\n",
            "\n",
            "=====Logistic Regression_DAI_ENGLISH=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.38      0.08      0.13       132\n",
            "     Neutral       0.72      0.96      0.82       679\n",
            "    Positive       0.53      0.16      0.24       167\n",
            "\n",
            "    accuracy                           0.70       978\n",
            "   macro avg       0.54      0.40      0.40       978\n",
            "weighted avg       0.64      0.70      0.63       978\n",
            "\n",
            "=====K Nearest Neighbor_DAI_ENGLISH=====\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.18      0.22      0.20       132\n",
            "     Neutral       0.69      0.82      0.75       679\n",
            "    Positive       0.58      0.04      0.08       167\n",
            "\n",
            "    accuracy                           0.61       978\n",
            "   macro avg       0.48      0.36      0.34       978\n",
            "weighted avg       0.60      0.61      0.56       978\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"====================== RESULTS FOR DAI ENGLISH TRANSALTED ==================\\n\")\n",
        "\n",
        "print(\"=====Support Vector Machine_DAI_ENGLISH=====\\n\")\n",
        "print(SVM_DAI_ENGLISH)\n",
        "\n",
        "print(\"=====Multinominal NB_UNQIUE_ENGLISH=====\\n\")\n",
        "print(MNB_DAI_ENGLISH)\n",
        "\n",
        "print(\"=====Logistic Regression_DAI_ENGLISH=====\\n\")\n",
        "print(LOGREG_DAI_ENGLISH)\n",
        "\n",
        "print(\"=====K Nearest Neighbor_DAI_ENGLISH=====\\n\")\n",
        "print(KNN_DAI_ENGLISH)\n",
        "\n",
        "# print(\"===== SGD_DAI_ENGLISH=====\\n\")\n",
        "# print(SGD_DAI_ENGLISH)\n",
        "\n",
        "# print(\"=====LINEARSVC_DAI_ENGLISH=====\\n\")\n",
        "# print(LINEARSVC_DAI_ENGLISH)\n",
        "\n",
        "# print(\"=====Bernoulli NB_DAI_ENGLISH=====\\n\")\n",
        "# print(BNB_DAI_ENGLISH)\n",
        "\n",
        "# print(\"=====Random Forest_DAI_ENGLISH=====\\n\")\n",
        "# print(RANFOR_DAI_ENGLISH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlNRX78y4yqg"
      },
      "source": [
        "Also tested how well combined dataset from SemEval and German_UNIQUE performs with traditional machine learning as it did well with BERT.\n",
        "\n",
        "=> IT DID NOT DO WELL WITH TML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZJiVe6ypqM2"
      },
      "source": [
        "# ====== PREDICTION COMPARISON WITH TRUE LABELS ======"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4cLirtS8LKF"
      },
      "source": [
        "# GERMAN VS ENGLISH TRANSLATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-aIO68L7ps0y"
      },
      "outputs": [],
      "source": [
        "English_translation_tweet = pd.DataFrame(German_translated[\"Tweet\"])\n",
        "English_translation_y_true = pd.DataFrame(German_translated_label)\n",
        "English_translation_y_predicted = pd.DataFrame(pred_SVC_SemEval_german)\n",
        "SVM_UNIGRAM_ENGLISH_TRANS_DATAFRAME = English_translation_y_true\n",
        "SVM_UNIGRAM_ENGLISH_TRANS_DATAFRAME[\"Predicted\"] = English_translation_y_predicted\n",
        "SVM_UNIGRAM_ENGLISH_TRANS_DATAFRAME[\"Tweet\"] = English_translation_tweet\n",
        "\n",
        "German_tweet = pd.DataFrame(German[\"Tweet\"])\n",
        "German_y_true = pd.DataFrame(German_label)\n",
        "German_y_predicted = pd.DataFrame(pred_SVC_DAI_german)\n",
        "\n",
        "SVM_UNIGRAM_GERMAN_DATAFRAME = German_y_true\n",
        "SVM_UNIGRAM_GERMAN_DATAFRAME[\"Predicted\"] = German_y_predicted\n",
        "SVM_UNIGRAM_GERMAN_DATAFRAME[\"Tweet\"] = German_tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_kkiYAF9-QKe",
        "outputId": "f5e36cd5-a42f-4475-faa6-f9f7226b9486"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5db5cb5d-c9ea-4b4c-be4c-6ec031a2f2a0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@Marcel126610 Irgendwie schon. Aber ...wer will ihn denn dann auch irgendwo rumsitzen haben...ü§∑‚Äç‚ôÄÔ∏è</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@PewPeeew aight nichts leichter als das</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>@LibertyHannes @RikeWaldfee @MarcoBuschmann Uhhhhh echt jetzt \" ich darf t√∂ten wenn ich will\" mimimi üò≠ und was mit euren Blagen ist, ist mir doch egal. Puh was ich dazu sage #FDPmachtkrankundarm  und #FDPunter5Prozent . Ihr seit auf einen guten Weg . üòÇ Ach #dielinke erholt sich gerade in den ersten Umfragen wider. üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Unter #IhrHabtEuchSelbstAusgegrenzt meinte gerade ein Querdenker,  es sollte doch konsequenzlos m√∂glich sein sich nicht impfen zu lassen.   Tja was soll ich sagen: F√ºr Pfleger und √Ñrzte am Ende ihrer Kr√§fte war es auch nicht konsequenzlos, wenn Querdenker die Impfung verweigern.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Und zur Sicherheit noch einmal: es geht hier um die Etikettierung, nicht das Testprodukt selbst. Kein Einfluss auf Ergebnis. https://t.co/Nne9nYSPgV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@MarcBrup @broennimann Welches Land findest Du inspirierend? (Ehrlich gemeinte Frage) Mich inspirieren Menschen, Natur, Musik, etc..aber ein ganzes Land?ü§î pS: ich bin sehr viel gereist, und k√∂nnte h√∂chstens sagen, dass die unterschiedlichsten Menschen und Kulturen inspirierend waren. Die Vielfalt.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>@besserossi @deprecatedCode @kotzlpotzl @drlisamaria Leichenfledderei! üò†</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>Ihr wurdet nie ausgegrenzt, ihr habt euch separiert  Ihr habt auf die Solidargemeinschaft geschissen  Ihr habt euch von Rechtsextremisten unterst√ºtzen lassen  Ihr werdet in Geschichtsb√ºchern nicht als Opfer sondern als T√§ter stehen  #wirhabenausgegrenzt #SolidaritaetmitderWoelfin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>Der Anwalt Chan-jo Jun hat Twitter verlassen. St√§ndig forderte er, dass wir mehr tun im Bereich Hasskriminalit√§t u. bekam sehr viele Morddrohungen. Verstehe, dass er jetzt genug hat. Tragisch ist sein R√ºckzug hier f√ºr unsere Demokratie dennoch. Hoffe sehr, er kommt zur√ºck.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G√ºnstiges #Russland-#Gas ist das beste Mittel gegen Verm√∂gensfra√ü beim B√ºrger. Stattdessen kommen #Gruene mit #Gasumlage und anderen #Grunen Plagen.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@kaktuskoeln @J_Todenhoefer Vielleicht kriege ich diese seltsamen Leute auch nur durcheinander.  Kann man sich ja nicht alle merken.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@moraimauy @ViktoriaGabor38 Welches Ort?Schreiben Sie bitte.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@DerHeuwer Ich hatte Pommes Spezial Spezial. Mayo, Hollandaise und Rostzwiebeln. Ich bin sehr gl√ºcklich ‚ò∫Ô∏è https://t.co/Ukx5UUuk1q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5db5cb5d-c9ea-4b4c-be4c-6ec031a2f2a0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5db5cb5d-c9ea-4b4c-be4c-6ec031a2f2a0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5db5cb5d-c9ea-4b4c-be4c-6ec031a2f2a0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Sentiment  Predicted  \\\n",
              "0           0          0   \n",
              "1           0          0   \n",
              "2          -1          0   \n",
              "3           0          0   \n",
              "4           0          0   \n",
              "5           1          0   \n",
              "6          -1          0   \n",
              "7          -1          0   \n",
              "8          -1          0   \n",
              "9           0          0   \n",
              "10          0          0   \n",
              "11          0          0   \n",
              "12          1          0   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                            Tweet  \n",
              "0                                                                                                                                                                                                                              @Marcel126610 Irgendwie schon. Aber ...wer will ihn denn dann auch irgendwo rumsitzen haben...ü§∑‚Äç‚ôÄÔ∏è  \n",
              "1                                                                                                                                                                                                                                                                                         @PewPeeew aight nichts leichter als das  \n",
              "2   @LibertyHannes @RikeWaldfee @MarcoBuschmann Uhhhhh echt jetzt \" ich darf t√∂ten wenn ich will\" mimimi üò≠ und was mit euren Blagen ist, ist mir doch egal. Puh was ich dazu sage #FDPmachtkrankundarm  und #FDPunter5Prozent . Ihr seit auf einen guten Weg . üòÇ Ach #dielinke erholt sich gerade in den ersten Umfragen wider. üòÇ  \n",
              "3                                         Unter #IhrHabtEuchSelbstAusgegrenzt meinte gerade ein Querdenker,  es sollte doch konsequenzlos m√∂glich sein sich nicht impfen zu lassen.   Tja was soll ich sagen: F√ºr Pfleger und √Ñrzte am Ende ihrer Kr√§fte war es auch nicht konsequenzlos, wenn Querdenker die Impfung verweigern.  \n",
              "4                                                                                                                                                                            Und zur Sicherheit noch einmal: es geht hier um die Etikettierung, nicht das Testprodukt selbst. Kein Einfluss auf Ergebnis. https://t.co/Nne9nYSPgV  \n",
              "5                      @MarcBrup @broennimann Welches Land findest Du inspirierend? (Ehrlich gemeinte Frage) Mich inspirieren Menschen, Natur, Musik, etc..aber ein ganzes Land?ü§î pS: ich bin sehr viel gereist, und k√∂nnte h√∂chstens sagen, dass die unterschiedlichsten Menschen und Kulturen inspirierend waren. Die Vielfalt.  \n",
              "6                                                                                                                                                                                                                                                        @besserossi @deprecatedCode @kotzlpotzl @drlisamaria Leichenfledderei! üò†  \n",
              "7                                        Ihr wurdet nie ausgegrenzt, ihr habt euch separiert  Ihr habt auf die Solidargemeinschaft geschissen  Ihr habt euch von Rechtsextremisten unterst√ºtzen lassen  Ihr werdet in Geschichtsb√ºchern nicht als Opfer sondern als T√§ter stehen  #wirhabenausgegrenzt #SolidaritaetmitderWoelfin  \n",
              "8                                               Der Anwalt Chan-jo Jun hat Twitter verlassen. St√§ndig forderte er, dass wir mehr tun im Bereich Hasskriminalit√§t u. bekam sehr viele Morddrohungen. Verstehe, dass er jetzt genug hat. Tragisch ist sein R√ºckzug hier f√ºr unsere Demokratie dennoch. Hoffe sehr, er kommt zur√ºck.  \n",
              "9                                                                                                                                                                            G√ºnstiges #Russland-#Gas ist das beste Mittel gegen Verm√∂gensfra√ü beim B√ºrger. Stattdessen kommen #Gruene mit #Gasumlage und anderen #Grunen Plagen.  \n",
              "10                                                                                                                                                                                           @kaktuskoeln @J_Todenhoefer Vielleicht kriege ich diese seltsamen Leute auch nur durcheinander.  Kann man sich ja nicht alle merken.  \n",
              "11                                                                                                                                                                                                                                                                   @moraimauy @ViktoriaGabor38 Welches Ort?Schreiben Sie bitte.  \n",
              "12                                                                                                                                                                                             @DerHeuwer Ich hatte Pommes Spezial Spezial. Mayo, Hollandaise und Rostzwiebeln. Ich bin sehr gl√ºcklich ‚ò∫Ô∏è https://t.co/Ukx5UUuk1q  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVM_UNIGRAM_GERMAN_DATAFRAME.head(13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iv-OCTOysEjr",
        "outputId": "42e99551-681a-4c41-fa77-68cfd8550dbb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1949fa83-a74d-49e9-89da-a74ac8b4d95b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@Marcel126610 Somehow yes. But...who wants it sitting around somewhere...ü§∑‚Äç‚ôÄÔ∏è</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>@PewPeeew aight nothing easier than that</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>@LibertyHannes @RikeWaldfee @MarcoBuschmann Uhhhhh really now \"I can kill if I want\" mimimi üò≠ and I don't care what's up with your brats. Puh what I say to #FDPmachtsickundarm and #FDPunter5Percent. You are on the right track. üòÇ Oh #dielinke is recovering in the first polls. üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>Under #YourHaveYouselfExcluded, a lateral thinker said that it should be possible not to be vaccinated without any consequences. Well, what can I say: For nurses and doctors at the end of their strength, it was not without consequences when lateral thinkers refused to be vaccinated.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>And to be on the safe side again: this is about the labeling, not the test product itself. No influence on the result. https://t.co/Nne9nYSPgV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>@MarcBrup @broennimann Which country do you find inspirational? (Honest question) I'm inspired by people, nature, music, etc.. but a whole country?ü§î PS: I've traveled a lot and could at most say that the most diverse people and cultures were inspiring. The diversity.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>@besserossi @deprecatedCode @kotzlpotzl @drlisamaria scavenging! üò†</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>You were never excluded, you separated. You gave a shit about the community of solidarity. You let right-wing extremists support you. You will not appear in history books as a victim but as a perpetrator #we have excluded #SolidaritaetmitderWoelfin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>Lawyer Chan-jo Jun has left Twitter. He kept demanding that we do more on hate crime and received many death threats. Understand that he's had enough now. His withdrawal here is still tragic for our democracy. I really hope he comes back.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Cheap #Russia #gas is the best remedy against the citizens' wealth being eaten up. Instead, #greens come with #gas surcharges and other #green plagues.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>@kaktuskoeln @J_Todenhoefer Maybe I'm just confusing these strange people. You can't remember them all.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@moraimauy @ViktoriaGabor38 Which place? Write please.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>@DerHeuwer I had fries special special. Mayo, hollandaise and fried onions. I am very happy ‚ò∫Ô∏è https://t.co/Ukx5UUuk1q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1949fa83-a74d-49e9-89da-a74ac8b4d95b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1949fa83-a74d-49e9-89da-a74ac8b4d95b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1949fa83-a74d-49e9-89da-a74ac8b4d95b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Sentiment  Predicted  \\\n",
              "0           0          0   \n",
              "1           0         -1   \n",
              "2          -1         -1   \n",
              "3           0         -1   \n",
              "4           0          0   \n",
              "5           1         -1   \n",
              "6          -1          0   \n",
              "7          -1         -1   \n",
              "8          -1         -1   \n",
              "9           0          0   \n",
              "10          0         -1   \n",
              "11          0          0   \n",
              "12          1          1   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                          Tweet  \n",
              "0                                                                                                                                                                                                                 @Marcel126610 Somehow yes. But...who wants it sitting around somewhere...ü§∑‚Äç‚ôÄÔ∏è  \n",
              "1                                                                                                                                                                                                                                                      @PewPeeew aight nothing easier than that  \n",
              "2         @LibertyHannes @RikeWaldfee @MarcoBuschmann Uhhhhh really now \"I can kill if I want\" mimimi üò≠ and I don't care what's up with your brats. Puh what I say to #FDPmachtsickundarm and #FDPunter5Percent. You are on the right track. üòÇ Oh #dielinke is recovering in the first polls. üòÇ  \n",
              "3   Under #YourHaveYouselfExcluded, a lateral thinker said that it should be possible not to be vaccinated without any consequences. Well, what can I say: For nurses and doctors at the end of their strength, it was not without consequences when lateral thinkers refused to be vaccinated.  \n",
              "4                                                                                                                                                And to be on the safe side again: this is about the labeling, not the test product itself. No influence on the result. https://t.co/Nne9nYSPgV  \n",
              "5                   @MarcBrup @broennimann Which country do you find inspirational? (Honest question) I'm inspired by people, nature, music, etc.. but a whole country?ü§î PS: I've traveled a lot and could at most say that the most diverse people and cultures were inspiring. The diversity.  \n",
              "6                                                                                                                                                                                                                            @besserossi @deprecatedCode @kotzlpotzl @drlisamaria scavenging! üò†  \n",
              "7                                      You were never excluded, you separated. You gave a shit about the community of solidarity. You let right-wing extremists support you. You will not appear in history books as a victim but as a perpetrator #we have excluded #SolidaritaetmitderWoelfin  \n",
              "8                                                Lawyer Chan-jo Jun has left Twitter. He kept demanding that we do more on hate crime and received many death threats. Understand that he's had enough now. His withdrawal here is still tragic for our democracy. I really hope he comes back.  \n",
              "9                                                                                                                                       Cheap #Russia #gas is the best remedy against the citizens' wealth being eaten up. Instead, #greens come with #gas surcharges and other #green plagues.  \n",
              "10                                                                                                                                                                                      @kaktuskoeln @J_Todenhoefer Maybe I'm just confusing these strange people. You can't remember them all.  \n",
              "11                                                                                                                                                                                                                                       @moraimauy @ViktoriaGabor38 Which place? Write please.  \n",
              "12                                                                                                                                                                       @DerHeuwer I had fries special special. Mayo, hollandaise and fried onions. I am very happy ‚ò∫Ô∏è https://t.co/Ukx5UUuk1q  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVM_UNIGRAM_ENGLISH_TRANS_DATAFRAME.head(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWVmyciy8OzH"
      },
      "source": [
        "# ENGLISH VS GERMAN TRANSLATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lc20FL3e8RzX"
      },
      "outputs": [],
      "source": [
        "English_tweet = pd.DataFrame(English['Tweet'])\n",
        "English_y_true = pd.DataFrame(English_label)\n",
        "English_y_pred = pd.DataFrame(pred_SVC_SemEval_english)\n",
        "SVM_UNIGRAM_ENGLISH_DATAFRAME = English_y_true\n",
        "SVM_UNIGRAM_ENGLISH_DATAFRAME['Predicted'] = English_y_pred\n",
        "SVM_UNIGRAM_ENGLISH_DATAFRAME['Tweet'] = English_tweet\n",
        "\n",
        "German_translation_tweet = pd.DataFrame(English_translated[\"Tweet\"])\n",
        "German_translation_y_true = pd.DataFrame(English_translated_label)\n",
        "German_translation_y_pred = pd.DataFrame(pred_SVC_german)\n",
        "\n",
        "SVM_UNIGRAM_GERMAN_TRANSLATION_DATAFRAME = German_translation_y_true\n",
        "SVM_UNIGRAM_GERMAN_TRANSLATION_DATAFRAME[\"Predicted\"] = German_translation_y_pred\n",
        "SVM_UNIGRAM_GERMAN_TRANSLATION_DATAFRAME[\"Tweet\"] = German_translation_tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_-NyhPRs_Q1e",
        "outputId": "ca859925-7d1b-4816-e457-03a7e65db3f7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2325d6f3-610a-4015-8321-23dc10ad9577\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>If you got ex drama please don‚Äôt hit me up ‚úåüèΩ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>If the World Bank wants to support the people, it must urge #Ethiopia to #EndTigraySiege and allow unhindered humanitarian access to #Tigray. #WorldBankStopFundingTigrayGenocide @SecBlinken @JosepBorrellF @WorldBank @UN_HRC @DavidMalpassWBG @IMFNews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Amusan and Duplantis, two athletes that broke records at Oregon 22 - https://t.co/z4ry6UPGgm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FACT https://t.co/0ZJlCLlytP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>220726 ¬© Êó∂Â∞öÂÖàÁîüEsquire ‚Äî ESQUIRE China shares a teaser image of Wu Lei on the upcoming cover of ESQUIRE‚Äôs August issue!   #Âê¥Á£ä ‚Ä¢ #wulei ‚Ä¢ #leowu ‚Ä¢ #‡∏≠‡∏π‡πã‡πÄ‡∏´‡∏•‡πà‡∏¢ ‡øê https://t.co/lbm2HKv0Df</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@_FacundoZapata Only listen to your soul!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@HibaycOfficial Good project with most potential and huge opportunity @malicknajim @FaridaZamou @JuniorS54441117 #NFTs #nftart #ETH #Hibayc #Airdrops</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Jai Nice got engaged and nobody knows who her man even is‚Ä¶ MOODü•Ç</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>The Employment Tribunal found that Garden Court Chambers discriminated against me because of my gender critical belief when it published a statement that I was under investigation and in upholding Stonewall‚Äôs complaint against me.   #AllisonBaileyWins</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>@fra27236945 @thatdayin1992 Why more than 80% of world population genuinely hate US/NATO/EU? Why do all of them so eagerly wait a moment of liberation?  Russia is not alone. 80% of world is on Russia's side.   The evil western empire will inevitably fall. The westerners cannot exploit the others for ever.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>@plussone Shit, and I've been doing this for free...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>someone saw haechan eating in a restaurant with jaemin today! he's wearing the jacket that mark gave to him &lt;3 https://t.co/QjV8mb6CBf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>Just hating the day ü•≤ https://t.co/qJiQaHnshD</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2325d6f3-610a-4015-8321-23dc10ad9577')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2325d6f3-610a-4015-8321-23dc10ad9577 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2325d6f3-610a-4015-8321-23dc10ad9577');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Sentiment  Predicted  \\\n",
              "0          -1          0   \n",
              "1           0          0   \n",
              "2           1          0   \n",
              "3           0          0   \n",
              "4           0          0   \n",
              "5           0          0   \n",
              "6           1          0   \n",
              "7           0          1   \n",
              "8          -1          0   \n",
              "9          -1         -1   \n",
              "10          0         -1   \n",
              "11          0          0   \n",
              "12         -1         -1   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                 Tweet  \n",
              "0                                                                                                                                                                                                                                                                        If you got ex drama please don‚Äôt hit me up ‚úåüèΩ  \n",
              "1                                                            If the World Bank wants to support the people, it must urge #Ethiopia to #EndTigraySiege and allow unhindered humanitarian access to #Tigray. #WorldBankStopFundingTigrayGenocide @SecBlinken @JosepBorrellF @WorldBank @UN_HRC @DavidMalpassWBG @IMFNews  \n",
              "2                                                                                                                                                                                                                         Amusan and Duplantis, two athletes that broke records at Oregon 22 - https://t.co/z4ry6UPGgm  \n",
              "3                                                                                                                                                                                                                                                                                         FACT https://t.co/0ZJlCLlytP  \n",
              "4                                                                                                                                  220726 ¬© Êó∂Â∞öÂÖàÁîüEsquire ‚Äî ESQUIRE China shares a teaser image of Wu Lei on the upcoming cover of ESQUIRE‚Äôs August issue!   #Âê¥Á£ä ‚Ä¢ #wulei ‚Ä¢ #leowu ‚Ä¢ #‡∏≠‡∏π‡πã‡πÄ‡∏´‡∏•‡πà‡∏¢ ‡øê https://t.co/lbm2HKv0Df  \n",
              "5                                                                                                                                                                                                                                                                            @_FacundoZapata Only listen to your soul!  \n",
              "6                                                                                                                                                                @HibaycOfficial Good project with most potential and huge opportunity @malicknajim @FaridaZamou @JuniorS54441117 #NFTs #nftart #ETH #Hibayc #Airdrops  \n",
              "7                                                                                                                                                                                                                                                     Jai Nice got engaged and nobody knows who her man even is‚Ä¶ MOODü•Ç  \n",
              "8                                                          The Employment Tribunal found that Garden Court Chambers discriminated against me because of my gender critical belief when it published a statement that I was under investigation and in upholding Stonewall‚Äôs complaint against me.   #AllisonBaileyWins  \n",
              "9   @fra27236945 @thatdayin1992 Why more than 80% of world population genuinely hate US/NATO/EU? Why do all of them so eagerly wait a moment of liberation?  Russia is not alone. 80% of world is on Russia's side.   The evil western empire will inevitably fall. The westerners cannot exploit the others for ever.  \n",
              "10                                                                                                                                                                                                                                                                @plussone Shit, and I've been doing this for free...  \n",
              "11                                                                                                                                                                              someone saw haechan eating in a restaurant with jaemin today! he's wearing the jacket that mark gave to him <3 https://t.co/QjV8mb6CBf  \n",
              "12                                                                                                                                                                                                                                                                       Just hating the day ü•≤ https://t.co/qJiQaHnshD  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVM_UNIGRAM_ENGLISH_DATAFRAME.head(13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To0xkKzg_VDD"
      },
      "outputs": [],
      "source": [
        "SVM_UNIGRAM_GERMAN_TRANSLATION_DATAFRAME.head(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsZU5LvnVkWO"
      },
      "source": [
        "# UNUSED FUNCTIONS ============================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asyJltfVVrBu"
      },
      "source": [
        "# UNUSED get tag\n",
        "\n",
        "Function that gets the POS of a given token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEidhufoVrBv"
      },
      "outputs": [],
      "source": [
        "#function to get the POS tag of a given token to achieve the optimal lemmatisation of the token\n",
        "def get_tag(token):\n",
        "    if token == \"VB\" or token == \"VBD\" or token == \"VBG\" or token == \"VBN\" or token == \"VBP\" or token == \"VBZ\": #VERBS\n",
        "        # print(\"It is a verb\")\n",
        "        return 'v'\n",
        "    elif token == \"JJ\" or token == \"JJR\" or token == \"JJS\": #ADJECTIVES\n",
        "        # print(\"Is it an adjective\")\n",
        "        return 'a'\n",
        "    elif token == \"RB\" or token == \"RBR\" or token == \"RBS\": #ADVERBS\n",
        "        # print(\"It is an adverb\")\n",
        "        return 'r'\n",
        "    elif token == \"NN\" or token == \"NNP\" or token == \"NNS\": #NOUNS\n",
        "        return 'n'\n",
        "    else:\n",
        "        # print(\"Unwanted\")\n",
        "        return \"unwanted tag\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJqRIw5AVrBw"
      },
      "source": [
        "# UNUSED base\n",
        "\n",
        "Function to get the base state of a given word through lemmatisation.\n",
        "\n",
        "Note: It fails at some lemmatisation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c03TujCOVrBw"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# function 'base' tokenises the input, then gets the POS tag of every token, then uses\n",
        "# the POS tag to determined how to accuartely lemmatise the token\n",
        "def base(text):\n",
        "\n",
        "# example = \"Peter is walking very slowly to the banks He is the slowest walker he will join us later Tom is better than Peter\"\n",
        "    text = word_tokenize(text) #tokenises the sentence\n",
        "# text = nltk.pos_tag(text, tagset='universal') #gets the pos tag fro each token in the text\n",
        "    text = nltk.pos_tag(text) #gets the pos tag fro each token in the text\n",
        "#  -> 'Peter', 'is', 'walking', 'to', 'the', 'banks', 'He', 'is', 'very', 'quick']\n",
        "# print(text)\n",
        "    lemmatized = \"\"\n",
        "    # print(\"Before lemmatizer process: \"+ lemmatized)\n",
        "    for token in text: \n",
        "        if get_tag(token[1]) == 'v':\n",
        "            # print(token[0])\n",
        "            lem_word = lemmatizer.lemmatize(token[0], 'v')\n",
        "            # print(\"VERB: \" +token[0], \"=>\", lem_word)\n",
        "            lemmatized += lem_word +\" \"\n",
        "            # print(\"VERB\")\n",
        "        elif get_tag(token[1]) == 'a':\n",
        "\n",
        "            lem_word = lemmatizer.lemmatize(token[0], 'a')\n",
        "            # print(\"ADJ: \" +token[0], \"=>\", lem_word)\n",
        "            lemmatized += lem_word +\" \"\n",
        "            # print(\"ADJECTIVE\")\n",
        "        elif get_tag(token[1]) == 'r':\n",
        "            # print(token[0])\n",
        "\n",
        "            lem_word = lemmatizer.lemmatize(token[0], 'r')\n",
        "            # print(\"ADV: \" +token[0], \"=>\", lem_word)\n",
        "            lemmatized += lem_word +\" \"\n",
        "            # print(\"ADVERB\")\n",
        "        elif get_tag(token[1]) == 'n':\n",
        "            lem_word = lemmatizer.lemmatize(token[0], 'n')\n",
        "            # print(\"NOUN: \" +token[0], \"=>\", lem_word)\n",
        "            lemmatized += lem_word +\" \"\n",
        "        else: \n",
        "            # lem_word = lemmatizer.lemmatize(token[0])\n",
        "            lemmatized += token[0] +\" \"\n",
        "\n",
        "        # get_tag(token[1])\n",
        "    return lemmatized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maMC7aVqVrBx"
      },
      "source": [
        "# UNUSED remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx04-dh2VrBx"
      },
      "outputs": [],
      "source": [
        "#fucntion ot remove ENGLISH stopwords from a text\n",
        "def removeENG_stopwords(text):\n",
        "    #gets the stop words for English from the nltk corpus\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    stop_words.remove(\"not\")\n",
        "    for word in stop_words:\n",
        "      text = re.sub(r'(?<!\\S)' + word + '+(?!\\S)', \"\", text, flags=re.IGNORECASE) #to change to a easier understandable function\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzc63dqYVrBz"
      },
      "source": [
        "# UNUSED Data Cleaning function\n",
        "\n",
        "This is a bit messy but seems to do the job. Could just import a library to clea the tweets up but wanted to see if I could do it myself\n",
        "\n",
        "Note: removeENG_stopwords disabled because it removing the stopwords will negatively impact scores by -1%. Not a lot but still a neagtive influence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5tU6lG9VrBz"
      },
      "outputs": [],
      "source": [
        "# variables and function calls to clean the data. \n",
        "# The issue probably lies here for the bad model performance but output texts seem fine?\n",
        "\n",
        "remove_symbols = '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])'  #emojis, symbols, links, and punctuation\n",
        "\n",
        "\n",
        "remove_spaces = ' +' #multiple spaces between words\n",
        "remove_RTandAT2 = '(RT |@\\w+|@ \\w+)' # RT and variants of @username : '@username', '@username_username' , '@ username'\n",
        "apostrophe = '&#39;'  # ASCI for an aposthrope \n",
        "apostrophe_csv = \"u2019\"  # ASCI for an aposthrope \n",
        "quote = '&quot;'     # ASCI for \"\" \n",
        "andSymbol = '&amp;|&' # ASCI for & \n",
        "greaterThan = '&gt;' # ASCI for > \n",
        "hashtag = '(#\\w+)' # hashtags \n",
        "double = '(.)\\1{2}' # duplicated characters\n",
        "dots = '\\.' # ellipsis\n",
        "numbers = '([\\d]+(?:st| st|nd| nd|rd| rd|th| th))|(\\d)' #cardinal numbers\n",
        "singleCHAR = '(?:^| )[b-hj-z](?= |$)' #matches single characters no 'a' or 'i; -> [b-hj-z]\n",
        "website = '(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
        "special = '[^\\w& ]+' # maches special characters except &\n",
        "multiple = \"(.)\\\\1{2,}\" #matches character duplicated 2 or more times \n",
        "\n",
        "def cleanENGText(text):\n",
        "    cleaned = re.sub(website, \"\", text)\n",
        "    cleaned = re.sub(apostrophe_csv, \"'\", cleaned) #? removes\\u2019; converting it to an apostrophe\n",
        "    cleaned = expand(cleaned)\n",
        "    cleaned = re.sub(remove_RTandAT2, \"\", cleaned) #?removes RT and variants of @username : '@username', '@username_username' , '@ username'\n",
        "    cleaned = re.sub(hashtag, \"\", cleaned)  #? replaces hashtags with nothing\n",
        "    cleaned = re.sub(special, \"\", cleaned)\n",
        "    cleaned = base(cleaned)\n",
        "    cleaned = re.sub(double, \"\", cleaned) #? removes multiple repeated characters until 2 left\n",
        "    cleaned = cleaned.lower() #? lower and trim trainling and leading whitespaces\n",
        "    cleaned = re.sub(quote, \"\\\"\", cleaned) #? removes &quot converting it to an apostrophe\n",
        "    cleaned = re.sub(andSymbol, \"and\", cleaned) #? replaces &amp; with 'and'\n",
        "    cleaned = re.sub(greaterThan, \"\", cleaned) #? replaces &gt; with nothing\n",
        "    # cleaned = removeENG_stopwords(cleaned) #? removes English stopwords\n",
        "    cleaned = re.sub(remove_symbols,\" \", cleaned) #? REMOVES EMOJIS, SYMBOLS, HTTPS, AND PUNCTUATION\n",
        "    cleaned = re.sub(remove_spaces,\" \", cleaned) #? removes multiple spaces  \n",
        "    cleaned = re.sub(numbers, \"\", cleaned) #? replaces numbers with nothing (could just use re.remove for these type of actions)\n",
        "    cleaned = re.sub(dots, \" \", cleaned) #? replaces dots with a space\n",
        "    cleaned = re.sub(singleCHAR, \"\", cleaned)\n",
        "    cleaned = re.sub(remove_spaces,\" \", cleaned) #? removes multiple spaces  \n",
        "    cleaned = re.sub(multiple, '\\\\1', cleaned) #? replaces multiple instances of character with 1 instance \n",
        "    cleaned = cleaned.strip()\n",
        "\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMJ5hEEik5Pp"
      },
      "source": [
        "# UNUSED BADLY PERFORMING MODELS ========================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mevNTEPblJFk"
      },
      "source": [
        "# SDGClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qZBzLHSlJFl"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# #SemEval alone\n",
        "# model_SemEval = SGDClassifier()\n",
        "# model_SemEval.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_SDG_SemEval = model_SemEval.predict(SemEval_test_tweet)\n",
        "# SGD_SemEval = classification_report(SemEval_test_label, pred_SDG_SemEval, target_names= classification_labels)\n",
        "\n",
        "# #SemEval on English labelled\n",
        "# model_SemEval_english = SGDClassifier()\n",
        "# model_SemEval_english.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_SDG_SemEval_english = model_SemEval_english.predict(English_tweet)\n",
        "# SGD_SemEval_ENGLISH = classification_report(English_label, pred_SDG_SemEval_english, target_names= classification_labels)\n",
        "\n",
        "# #SemEval on German labelled tranlsated into English\n",
        "# model_SemEval_german = SGDClassifier()\n",
        "# model_SemEval_german.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_SDG_SemEval_german = model_SemEval_german.predict(German_translated_tweet)\n",
        "# SGD_SemEval_GERMAN = classification_report(German_translated_label, pred_SDG_SemEval_german, target_names= classification_labels)\n",
        "\n",
        "# #DAI alone\n",
        "# model_DAI = SGDClassifier()\n",
        "# model_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_SDG_DAI = model_DAI.predict(DAI_test_tweet)\n",
        "# SGD_DAI = classification_report(DAI_test_label, pred_SDG_DAI, target_names= classification_labels)\n",
        "\n",
        "# #DAI on German\n",
        "# model_DAI_german = SGDClassifier()\n",
        "# model_DAI_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_SDG_DAI_german = model_DAI_german.predict(German_tweet)\n",
        "# SGD_DAI_GERMAN = classification_report(German_label, pred_SDG_DAI_german, target_names= classification_labels)\n",
        "\n",
        "# #DAI on English labelled transalted into German\n",
        "# model_DAI = SGDClassifier()\n",
        "# model_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_SDG_German = model_DAI.predict(English_translated_tweet)\n",
        "# SGD_DAI_ENGLISH  = classification_report(English_translated_label, pred_SDG_German, target_names= classification_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j91mFt4IlJFl"
      },
      "source": [
        "# LINEAR SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PFBZqfXlJFm"
      },
      "outputs": [],
      "source": [
        "# #SemEval alone\n",
        "# clf_SemEval = LinearSVC(max_iter= 1000)\n",
        "# clf_SemEval.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_SVC_SemEval = clf_SemEval.predict(SemEval_test_tweet)\n",
        "# LINEARSVC_SemEval = classification_report(SemEval_test_label, pred_SVC_SemEval, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel],\n",
        "\n",
        "# #SemEval on English labelled\n",
        "# clf_SemEval_english = LinearSVC(max_iter= 1000)\n",
        "# clf_SemEval_english.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_SVC_SemEval_english = clf_SemEval_english.predict(English_tweet)\n",
        "# LINEARSVC_SemEval_ENGLISH = classification_report(English_label, pred_SVC_SemEval_english, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel],\n",
        "\n",
        "# #SemEval on German labelled tranlsated into English\n",
        "# clf_SemEval_german = LinearSVC(max_iter= 1000)\n",
        "# clf_SemEval_german.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_SVC_SemEval_german = clf_SemEval_german.predict(German_translated_tweet)\n",
        "# LINEARSVC_SemEval_GERMAN = classification_report(German_translated_label, pred_SVC_SemEval_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel],\n",
        "\n",
        "# #DAI alone\n",
        "# clf_DAI = LinearSVC(max_iter= 1000)\n",
        "# clf_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_SVC_DAI = clf_DAI.predict(DAI_test_tweet)\n",
        "# LINEARSVC_DAI = classification_report(DAI_test_label, pred_SVC_DAI, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel],\n",
        "\n",
        "# #DAI on German\n",
        "# clf_DAI_german = LinearSVC(max_iter= 1000)\n",
        "# clf_DAI_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_SVC_DAI_german = clf_DAI.predict(German_tweet)\n",
        "# LINEARSVC_DAI_GERMAN = classification_report(German_label, pred_SVC_DAI_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel],\n",
        "\n",
        "# #DAI on English labelled transalted into German\n",
        "# clf_german = LinearSVC(max_iter= 1000)\n",
        "# clf_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_SVC_German = clf_german.predict(English_translated_tweet)\n",
        "# LINEARSVC_DAI_ENGLISH  = classification_report(English_translated_label, pred_SVC_German, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel],\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjvovxp_lJFm"
      },
      "source": [
        "#  Bernoulli Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K86DhBRlJFm"
      },
      "outputs": [],
      "source": [
        "# #SemEval alone\n",
        "# BNBmodel_SemEval = BernoulliNB()\n",
        "# BNBmodel_SemEval.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_BNB_SemEval = BNBmodel_SemEval.predict(SemEval_test_tweet)\n",
        "# BNB_SemEval = classification_report(SemEval_test_label, pred_BNB_SemEval, target_names= classification_labels)\n",
        "\n",
        "# #SemEval on English labelled\n",
        "# BNBmodel_SemEval_english = BernoulliNB()\n",
        "# BNBmodel_SemEval_english.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_BNB_SemEval_english = BNBmodel_SemEval_english.predict(English_tweet)\n",
        "# BNB_SemEval_ENGLISH = classification_report(English_label, pred_BNB_SemEval_english, target_names= classification_labels)\n",
        "\n",
        "# #SemEval on German labelled tranlsated into English\n",
        "# BNBmodel_SemEval_german = BernoulliNB()\n",
        "# BNBmodel_SemEval_german.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_BNB_SemEval_german = BNBmodel_SemEval_german.predict(German_translated_tweet)\n",
        "# BNB_SemEval_GERMAN = classification_report(German_translated_label, pred_BNB_SemEval_german, target_names= classification_labels)\n",
        "\n",
        "# #German_DAI alone\n",
        "# BNBmodel_DAI = BernoulliNB()\n",
        "# BNBmodel_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_BNB_DAI = BNBmodel_DAI.predict(DAI_test_tweet)\n",
        "# BNB_DAI = classification_report(DAI_test_label, pred_BNB_DAI, target_names= classification_labels)\n",
        "\n",
        "# #German_DAI on German\n",
        "# BNBmodel_DAI = BernoulliNB()\n",
        "# BNBmodel_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_BNB_DAI = BNBmodel_DAI.predict(German_tweet)\n",
        "# BNB_DAI_GERMAN = classification_report(German_label, pred_BNB_DAI, target_names= classification_labels)\n",
        "\n",
        "\n",
        "# #German_DAI on English labelled transalted into German\n",
        "# BNBmodel_german = BernoulliNB()\n",
        "# BNBmodel_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_BNB_German = BNBmodel_german.predict(English_translated_tweet)\n",
        "# BNB_DAI_ENGLISH  = classification_report(English_translated_label, pred_BNB_German, target_names= classification_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLcuOwM7lgLs"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNaYrwRklgLt"
      },
      "outputs": [],
      "source": [
        "# #SemEval alone\n",
        "# rfc_SemEval = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "# rfc_SemEval.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_rfc_SemEval = rfc_SemEval.predict(SemEval_test_tweet)\n",
        "# RANFOR_SemEval = classification_report(SemEval_test_label, pred_rfc_SemEval, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "# #SemEval on English labelled\n",
        "# rfc_SemEval_english = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "# rfc_SemEval_english.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_rfc_SemEval_english = rfc_SemEval_english.predict(English_tweet)\n",
        "# RANFOR_SemEval_ENGLISH = classification_report(English_label, pred_rfc_SemEval_english, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "# #SemEval on German labelled tranlsated into English\n",
        "# rfc_SemEval_german = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "# rfc_SemEval_german.fit(SemEval_train_tweet, SemEval_train_label)\n",
        "# pred_rfc_SemEval_german = rfc_SemEval_german.predict(German_translated_tweet)\n",
        "# RANFOR_SemEval_GERMAN = classification_report(German_translated_label, pred_rfc_SemEval_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "# #DAI alone\n",
        "# rfc_DAI = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "# rfc_DAI.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_rfc_DAI = rfc_DAI.predict(DAI_test_tweet)\n",
        "# RANFOR_DAI = classification_report(DAI_test_label, pred_rfc_DAI, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "# #DAI on German\n",
        "# rfc_DAI_german = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "# rfc_DAI_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_rfc_DAI_german = rfc_DAI.predict(German_tweet)\n",
        "# RANFOR_DAI_GERMAN = classification_report(German_label, pred_rfc_DAI_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "\n",
        "# #DAI on English labelled transalted into German\n",
        "# rfc_german = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "# rfc_german.fit(DAI_train_tweet, DAI_train_label)\n",
        "# pred_rfc_german = rfc_german.predict(English_translated_tweet)\n",
        "# RANFOR_DAI_ENGLISH = classification_report(English_translated_label, pred_rfc_german, target_names= classification_labels) #actual testLabel, y_pred  [i.e predicyed testLabel], \n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "asyJltfVVrBu",
        "IJqRIw5AVrBw",
        "maMC7aVqVrBx",
        "jzc63dqYVrBz",
        "mevNTEPblJFk",
        "j91mFt4IlJFl",
        "Gjvovxp_lJFm",
        "QLcuOwM7lgLs"
      ],
      "machine_shape": "hm",
      "name": "Traditional Machine Learning Models.ipynb",
      "provenance": [],
      "mount_file_id": "130Aor2O1220hdQUsiwPPSyti4uWxkFOc",
      "authorship_tag": "ABX9TyPK3x+7pbYMEoTgp9jGuI4b",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}